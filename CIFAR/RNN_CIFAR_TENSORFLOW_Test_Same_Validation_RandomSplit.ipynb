{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr\n",
        "!pip install flwr[simulation]\n",
        "!pip install tensorflow[and-cuda]\n",
        "!pip install tensorflow_datasets\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "4kIu7Hl7V8gd",
        "outputId": "a3b48578-6406-4722-ba91-bb77c575907d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (41.0.7)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.60.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.23.5)\n",
            "Collecting protobuf<5.0.0,>=4.25.2 (from flwr)\n",
            "  Using cached protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (3.20.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr) (2.21)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.2 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.2\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (41.0.7)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.60.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.23.5)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (4.25.2)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (3.20.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.10.14)\n",
            "Requirement already satisfied: ray==2.6.3 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (2.6.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (23.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray==2.6.3->flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr[simulation]) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->flwr[simulation]) (4.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr[simulation]) (2.21)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.6.3->flwr[simulation]) (0.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.6.3->flwr[simulation]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.6.3->flwr[simulation]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.6.3->flwr[simulation]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.6.3->flwr[simulation]) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.15.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.2.5.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.5.6)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.2.142 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.142)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.2.140 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.140)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.2.140 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.140)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.2.140 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.140)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.4.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (8.9.4.25)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.8.103 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (11.0.8.103)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.3.141 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (10.3.3.141)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.5.2.141 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (11.5.2.141)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.2.141 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.1.2.141)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.16.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.16.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.2.140 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.140)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (3.5.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow[and-cuda])\n",
            "  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow[and-cuda]) (3.2.2)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.2\n",
            "    Uninstalling protobuf-4.25.2:\n",
            "      Successfully uninstalled protobuf-4.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flwr 1.7.0 requires protobuf<5.0.0,>=4.25.2, but you have protobuf 4.23.4 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.23.4\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.23.5)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.23.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.1.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.9.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2024.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow_datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.62.0)\n",
            "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
            "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.23.4\n",
            "    Uninstalling protobuf-4.23.4:\n",
            "      Successfully uninstalled protobuf-4.23.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flwr 1.7.0 requires protobuf<5.0.0,>=4.25.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sI6pW4jCV3fL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "import flwr as fl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "from flwr.common import Metrics\n",
        "from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n",
        "enable_tf_gpu_growth()\n",
        "#https://medium.com/mlearning-ai/evaluating-federated-learning-from-felt-labs-on-mnist-dataset-cbe081b28786\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDfowKgcV3fM"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HwsY4Sw6V3fN"
      },
      "outputs": [],
      "source": [
        "def getcifarData():\n",
        "    (traindataset,testDataset),datasetinfo = tfds.load(\n",
        "        'cifar10',\n",
        "        split=['train','test'],\n",
        "        shuffle_files= True,\n",
        "        as_supervised= True,\n",
        "        with_info= True\n",
        "    )\n",
        "    return traindataset,testDataset,datasetinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KT-gMIzvV3fN"
      },
      "outputs": [],
      "source": [
        "def dataset_to_numpy(dataset):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for image, label in tfds.as_numpy(dataset):\n",
        "        features.append(image)\n",
        "        labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "def Splitset(intrain,intest):\n",
        "    Train_images, Train_label = dataset_to_numpy(intrain)\n",
        "    Test_images, Test_label = dataset_to_numpy(intest)\n",
        "    return  Train_images, Train_label,Test_images, Test_label\n",
        "\n",
        "def datanorm(data):\n",
        "    return data/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_p4JKHp7V3fN",
        "outputId": "f0943b94-f9b0-4a11-c3b4-50cf5339c2c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "f67caa4bb70d417ca8f95dec68ea56a1",
            "aa716e3d81a24313a8348b71396bebb8",
            "2de9200a359941ecb9eaf4e014784368",
            "2593234433b742f59ffb279360972ed4",
            "34c7700212e3429bb27a22b9bf3558fd",
            "fec6e7ccd6bc4eecac23b7da472edaa8",
            "78e84ddb2db348488dc3e551d075e718",
            "06e5ee7bae6442dda9ea831f8e762881",
            "7b2e54acd915432a8d265d5b8fef5192",
            "02bb9e7765b34cc0a534c5b09be0a9d8",
            "23115e2a727b425d955a063e8c577e7d",
            "b029d230fc6841bf958fe62008c287b5",
            "f7151dd7d59c482582b96208421544ee",
            "cc09c3bc809a411aaadc18c90a7ab8a3",
            "45e07534d4784bd1b2584161687641d0",
            "405fff3894f749cb8a6004b626da43d5",
            "fc46c385f9084eaaaaf4fa267d0ddf18",
            "2c93b55be9674e0ea6e81d909e2ff691",
            "ff250aaf9faf49a5825ea3771efef8e9",
            "2163f8c61f774ba9ab807833580cad1d",
            "45c07efe02104511a4694382ef9a64ce",
            "ec49346a96a44e1fb385cba80eeb5cad",
            "3d1fe72e89b34a3393c997a199472025",
            "2aa07bb54c0b4adaa157e7656e238f66",
            "cba1e40517ae43fcab0040d51f8d953c",
            "6c4da78ad379407caac7117313aae9e0",
            "8fd070ce820244f8a837c49ea375a92e",
            "d7a42d8287c9429f930dfc420ddecac5",
            "05895b42051943c086a9fea62392f8ec",
            "52d60d20889a4f059a20045473d072e9",
            "aec203a57e4347e0a6fcc91bfce218ed",
            "06d9dbaad4aa460eb3f0520d78f231e0",
            "6034510f50f14b2687d76d9a3b9f5851",
            "fca8b13e7793406190eb4aaad9ed2a36",
            "188ffdde4e274022a8553ebe77a2c438",
            "7253d3af22a44d7ba186f4604a44a603",
            "ff7f1e9e3b8149aabd6274ad16de49df",
            "569a575c23044b88bf0bfc257fd4e084",
            "02f9f7e73a904f13a4eb86c07785eb71",
            "fbda1a6d1a0b454da9b565a72b9be49b",
            "5c3f9c467fcb4d709d4fba1d3478cf39",
            "641b46737fdc4abfbdbbeefce69efcc1",
            "3f596042cc274621bcb38610419c82a0",
            "29d6cfff5ecd4dc492d48e7d021e8f00",
            "2afa12ac215244d1bf0f557f90ad0ba8",
            "63f2f97cfb46483f8d6af148e1596beb",
            "80dfc07a4da64adf81d1e9280ee85932",
            "de86a2e034c04b90b8682667574b0b97",
            "fa513f703f764d1cb37965d23eaecc5f",
            "1bee3a112e494c0a90ff2e5cd8a059cb",
            "8c508789f68243748fa0ea74f9303fb7",
            "cd76fbe28eec4c43a5cb8c3c54d594b3",
            "ed2742f07fe042869f8825acca9e2b16",
            "2ed2adfd49b143bc82868e67e3340a11",
            "46b2e1302a9048a18894a5a67c343034",
            "34c35fd2e02b4a1eaad91fda32d70673",
            "4d2a4658ade04437946cc08dd18c7cf8",
            "cb8de5adf4204c8ba410bd897fa6828f",
            "b9d40ec0f9ea4f47bf17a1249d5338e3",
            "ca97a277372c48508273ed403ee40386",
            "620c4ed423624ee39a9fc2b2e94aba55",
            "75a698042013485db6013b46719a362b",
            "cd8227378398453987db29c79c15e5f0",
            "8b8bdad723154e3ca712bc6df3248f79",
            "007ba4e2373f48e18956ba6130e5d421",
            "13070cbe16724c8e9982c06dc6d4e22e",
            "f898fc464b1c49018fbcd9ae72648c99",
            "d7308710cbfd45a9af7a272014920f03",
            "fd3b7c5e8aba4453960bbca2caf9d712",
            "26d970865de74f3d9d1a6263e0a7b340",
            "4a6b141318af425bac8006eeb8c381ce",
            "e5e21744327942e1af6458a190f784f8",
            "db66c0e67d284555bcec640af2418856",
            "82301467a6ad4b0eac2e6363e35ffa2d",
            "e5d8cd7702294e32a85f5531ba76cb05",
            "d01f6156c5024f69ab25bc44a5510e45",
            "f2e8f66131f44d8fb5743d9ac0e125ab",
            "ef36693bae7f40b9b7c733d0ef8a828b",
            "255d24d087784f0d9f9838573a2ad8b5",
            "36c1b7cd10774474bb055c6cc7c24e15",
            "b27f601bba3a4d39ab576ebe97e930aa",
            "26cd62668f794512a0ba325d052339dc",
            "e2b87a4fb1534b87a854f7799826fae2",
            "458fbf087baa430a97874adbbb50e23f",
            "a862c943c859472bb05230a78880dce4",
            "bc7b9e9f66f340f5ae5e5c0a4b2a273a",
            "7181073e976942388e12662c90e14d2c",
            "8e40f480f0904d7a9cc6072f961dc92f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /root/tensorflow_datasets/cifar10/3.0.2...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f67caa4bb70d417ca8f95dec68ea56a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b029d230fc6841bf958fe62008c287b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d1fe72e89b34a3393c997a199472025"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca8b13e7793406190eb4aaad9ed2a36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2afa12ac215244d1bf0f557f90ad0ba8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/cifar10/3.0.2.incompleteZHGCAF/cifar10-train.tfrecord*...:   0%|          …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c35fd2e02b4a1eaad91fda32d70673"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f898fc464b1c49018fbcd9ae72648c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/cifar10/3.0.2.incompleteZHGCAF/cifar10-test.tfrecord*...:   0%|          |…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef36693bae7f40b9b7c733d0ef8a828b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cifar10 downloaded and prepared to /root/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "trainset,testset, info = getcifarData()\n",
        "Train_images, Train_label,Test_images, Test_label = Splitset(trainset,testset)\n",
        "Train_images_n = datanorm(Train_images)\n",
        "Test_images_n = datanorm(Test_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_XwOkWm4V3fN",
        "outputId": "1f3a192a-af87-403c-f677-19674f5e1481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig_1,axs_1 = plt.subplots(1,2,figsize = (15,4))\n",
        "axs_1[0].hist(Train_label, bins=range(11), align='left', rwidth=0.8, color='#1f77b4', edgecolor='black',label='Train')\n",
        "axs_1[0].set_title('Distribution of CIFAR Train Labels')\n",
        "axs_1[0].set_xlabel('Label')\n",
        "axs_1[0].set_ylabel('Sample')\n",
        "axs_1[0].set_xticks(range(10))  # Digits 0 to 9\n",
        "axs_1[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "axs_1[1].hist(Test_label, bins=range(11), align='left', rwidth=0.8, color='#1f77b4', edgecolor='black',label='Train')\n",
        "axs_1[1].set_title('Distribution of CIFAR Test Labels')\n",
        "axs_1[1].set_xlabel('Label')\n",
        "axs_1[1].set_ylabel('Sample')\n",
        "axs_1[1].set_xticks(range(10))  # Digits 0 to 9\n",
        "axs_1[1].grid(axis='y', linestyle='--', alpha=0.7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRvG6Y6XV3fO"
      },
      "source": [
        "# Partition DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8yR_TjJZV3fO"
      },
      "outputs": [],
      "source": [
        "def split_into_partition(data, labels, num_partitions):\n",
        "    # Combine data and labels\n",
        "    combined_data = list(zip(data, labels))\n",
        "    np.random.shuffle(combined_data)  # Shuffle the combined data randomly\n",
        "\n",
        "    partitions = [[] for _ in range(num_partitions)]\n",
        "    store_label = np.unique(labels).tolist()\n",
        "    counter = 0\n",
        "    count = 0\n",
        "    # Iterate over unique labels to efficiently distribute them across partitions\n",
        "    while store_label:\n",
        "        random_choice = np.random.choice(store_label)\n",
        "\n",
        "        # Extract data and labels for the selected class\n",
        "        class_data = [item[0] for item in combined_data if item[1] == random_choice]\n",
        "        class_labels = [random_choice] * len(class_data)\n",
        "\n",
        "        # Distribute the class data evenly among partitions\n",
        "        modnum = len(np.unique(labels)) // num_partitions\n",
        "\n",
        "        if counter == (num_partitions - 1):\n",
        "            partitions[counter].extend(list(zip(class_data, class_labels)))\n",
        "        elif count <= modnum:\n",
        "            # Extend the selected partition with the data and labels\n",
        "            count+=1\n",
        "            partitions[counter].extend(list(zip(class_data, class_labels)))\n",
        "            if count == modnum:\n",
        "                counter += 1\n",
        "                count=0\n",
        "\n",
        "\n",
        "        store_label.remove(random_choice)\n",
        "\n",
        "   # Separate data and labels after partitioning\n",
        "    data_partitions = [np.array(list(zip(*partition))[0]) if partition else np.array([]) for partition in partitions]\n",
        "    label_partitions = [np.array(list(zip(*partition))[1]) if partition else np.array([]) for partition in partitions]\n",
        "\n",
        "\n",
        "    return data_partitions, label_partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tVhFk4rkV3fO"
      },
      "outputs": [],
      "source": [
        "NumOfPartition = 10\n",
        "train_image_part,train_label_part = split_into_partition(Train_images_n,Train_label,NumOfPartition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TQowQ9pCV3fO",
        "outputId": "69462bc7-a454-4a3e-a821-8fe5a5dbf5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot histogram for each partition\n",
        "fig, axs = plt.subplots(NumOfPartition, sharex=True, sharey=True)\n",
        "\n",
        "for i, partition in enumerate(train_label_part):\n",
        "    axs[i].hist(partition, bins=range(11), align='left', rwidth=0.8, color='#1f77b4', edgecolor='black')\n",
        "    axs[i].set_title(f'Partition {i + 1}')\n",
        "    axs[i].set_xlabel('Label')\n",
        "    axs[i].set_ylabel('Sample')\n",
        "    axs[i].set_xticks(range(10))\n",
        "    axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.suptitle('Distribution of Labels in Train Partitions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3IcGCB7JV3fO",
        "outputId": "d50c8fab-268d-4475-acb9-bd083d94485f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYw0lEQVR4nO3de1wUhd4/8M/KHZaLgMuyiKhgi2le0jIvHTUMDyoVHQ3DDqCVZXrSFDU61QKlpKWhpmbWo2XwaJlap7ykZna1zCIrsdREQGCR2wKKGjC/P/y5jyuguyvLXM7n/XrNq3Zmdub72dldv8xlRyUIggAiIiIiGeogdgFERERE9mIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjI0NpaWlQqVTtsq4RI0ZgxIgR5seff/45VCoVNm/e3C7rT05ORteuXdtlXfaqq6vDI488Aq1WC5VKhVmzZrXLepOTk6FWq9t0mVdvb3vl5+dDpVJh/fr1N7ys9qRSqZCWliZ2GW3Klm0qh8/b4sWLERkZiaamJrFLka077rgD8+bNE7uMNsNGRmTr16+HSqUyD+7u7tDpdBg9ejSWL1+O2traNllPcXEx0tLSkJub2ybLa0tSrs0aCxcuxPr16zFt2jRs2LAB//znP1udt2vXrhg3blw7Vid/V39GWhuk+A9wcnKyRY0+Pj7o27cvlixZggsXLrTZeo4cOYK0tDTk5+dfd145f95qamqwaNEizJ8/Hx06/N8/XyqVCjNmzGjxOZffPz/88IPD6pLbazp//nysXLkSpaWlYpfSJpzFLoAuycjIQLdu3fDXX3+htLQUn3/+OWbNmoWlS5fio48+Qp8+fczzPvvss3j66adtWn5xcTHS09PRtWtX9OvXz+rnffrppzatxx7Xqm3t2rWS/8vrs88+wx133AGDwSB2KZIRFhaG+vp6uLi43PCy/va3v2HDhg0W4x555BHcfvvtmDp1qnlcW+ydqq+vh7Nz234turm54c033wQAVFdX44MPPkBKSgoOHjyIjRs3tsk6jhw5gvT0dIwYMaJZQ3f1Z1jOn7f/+Z//QUNDAx588EGxS7Fg7/erWO699174+Phg1apVyMjIELucG8ZGRiJiYmIwcOBA8+PU1FR89tlnGDduHO655x7k5eXBw8MDAODs7NzmX7ZXO3fuHDw9PeHq6urQ9VxPW/xD6GhlZWW4+eabxS5DUi7vXWwL3bt3R/fu3S3GPf744+jevTseeuihVp/X0NCApqYmm97DbVXzlZydnS3qfOKJJzBo0CBs2rQJS5cuhU6ns3vZ58+fv24+W/JL/fO2bt063HPPPQ7ZTv9NOnTogPHjx+Odd95Benp6u52q4Cg8tCRhd911F5577jmcOnUK7777rnl8S+fI7N69G8OGDYOfnx/UajX0ej2eeeYZAJfOa7ntttsAAJMnTzbv5r58/sKIESPQu3dvHDp0CH/729/g6elpfm5rx9cbGxvxzDPPQKvVwsvLC/fccw8KCwst5unatSuSk5ObPffKZV6vtpaO2Z89exZz5sxBaGgo3NzcoNfr8corr+DqG7lf3t28bds29O7dG25ubujVqxd27tzZ8gt+lbKyMjz88MMICgqCu7s7+vbti7fffts8/fL5QidPnsQnn3xirt2a3fvX8uWXX2LChAno0qUL3NzcEBoaiqeeegr19fUtzv/nn39i9OjR8PLygk6nQ0ZGRrPXoqmpCVlZWejVqxfc3d0RFBSExx57DFVVVdetZ8WKFejVqxc8PT3RsWNHDBw4EDk5Odd8TkvnyFw+p+f06dO47777oFar0alTJ6SkpKCxsfH6L4wV63vllVeQlZWF8PBwuLm54ciRI7h48SKef/55DBgwAL6+vvDy8sKdd96Jffv2NVvO1efIXP6sHT9+HMnJyfDz84Ovry8mT56Mc+fO2VVrhw4dzO///Px8VFZWIiUlBbfccgvUajV8fHwQExODn3/+2eJ5l99vGzduxLPPPouQkBB4enpi+fLlmDBhAgBg5MiR5vfh559/DkB6n7fa2lrMmjULXbt2hZubGzQaDe6++278+OOP13zdTp48icOHD2PUqFHWvMzXdfToUYwfPx7+/v5wd3fHwIED8dFHH1nMY822sfb79fDhwxg+fDg8PT0RERFhPs9w//79GDRoEDw8PKDX67Fnzx6LGk6dOoUnnngCer0eHh4eCAgIwIQJE5p9z1w+hPbFF1/gscceQ0BAAHx8fJCYmNji5/zuu+/GqVOnZHM47Fq4R0bi/vnPf+KZZ57Bp59+ikcffbTFeX777TeMGzcOffr0QUZGBtzc3HD8+HF8/fXXAICePXsiIyMDzz//PKZOnYo777wTADBkyBDzMioqKhATE4OJEyfioYceQlBQ0DXrWrBgAVQqFebPn4+ysjJkZWVh1KhRyM3NNe85soY1tV1JEATcc8892LdvHx5++GH069cPu3btwty5c3H69Gm8+uqrFvN/9dVX2LJlC5544gl4e3tj+fLl+Mc//oGCggIEBAS0Wld9fT1GjBiB48ePY8aMGejWrRvef/99JCcno7q6GjNnzkTPnj2xYcMGPPXUU+jcuTPmzJkDAOjUqZPV+Vvy/vvv49y5c5g2bRoCAgLw/fffY8WKFSgqKsL7779vMW9jYyP+/ve/44477sDixYuxc+dOGAwGNDQ0WOwyfuyxx7B+/XpMnjwZTz75JE6ePInXXnsNP/30E77++utW/xJfu3YtnnzySYwfPx4zZ87E+fPncfjwYXz33XdISEiwOVtjYyNGjx6NQYMG4ZVXXsGePXuwZMkShIeHY9q0aTYv72rr1q3D+fPnMXXqVLi5ucHf3x81NTV488038eCDD+LRRx9FbW0t3nrrLYwePRrff/+9VYcCHnjgAXTr1g2ZmZn48ccf8eabb0Kj0WDRokV21XnixAkAQEBAAP78809s27YNEyZMQLdu3WA0GrFmzRoMHz4cR44cabbH5oUXXoCrqytSUlJw4cIFREdH48knn8Ty5cvxzDPPoGfPngBg/u+VpPB5e/zxx7F582bMmDEDN998MyoqKvDVV18hLy8Pt956a6uv2TfffAMArc5z/vx5lJeXNxtfV1fXbNxvv/2GoUOHIiQkBE8//TS8vLzw3nvv4b777sMHH3yAuLg4ALBq21jzmlZVVWHcuHGYOHEiJkyYgNWrV2PixInIzs7GrFmz8PjjjyMhIQEvv/wyxo8fj8LCQnh7ewMADh48iG+++QYTJ05E586dkZ+fj9WrV2PEiBE4cuQIPD09LbLNmDEDfn5+SEtLw++//47Vq1fj1KlT5kb4sgEDBgAAvv76a/Tv37/V110WBBLVunXrBADCwYMHW53H19dX6N+/v/mxwWAQrtx0r776qgBAOHPmTKvLOHjwoABAWLduXbNpw4cPFwAIr7/+eovThg8fbn68b98+AYAQEhIi1NTUmMe/9957AgBh2bJl5nFhYWFCUlLSdZd5rdqSkpKEsLAw8+Nt27YJAIQXX3zRYr7x48cLKpVKOH78uHkcAMHV1dVi3M8//ywAEFasWNFsXVfKysoSAAjvvvuuedzFixeFwYMHC2q12iJ7WFiYMHbs2Gsuz5Z5z50712xcZmamoFKphFOnTpnHJSUlCQCEf/3rX+ZxTU1NwtixYwVXV1fz++HLL78UAAjZ2dkWy9y5c2ez8Vdvm3vvvVfo1auXVdmudPLkyWbb9HK9GRkZFvP2799fGDBggE3L9/LysnhvXV6fj4+PUFZWZjFvQ0ODcOHCBYtxVVVVQlBQkDBlyhSL8QAEg8Fgfnz5s3b1fHFxcUJAQMB160xKShK8vLyEM2fOCGfOnBGOHz8uLFy4UFCpVEKfPn0EQRCE8+fPC42NjRbPO3nypODm5mbxWl3+7HXv3r3Ze+T9998XAAj79u1rVoPUPm++vr7C9OnTm79Y1/Hss88KAITa2tpm0wBcd7jyOzYqKkq45ZZbhPPnz5vHNTU1CUOGDBF69OhhHmfttrHm+zUnJ8c87ujRowIAoUOHDsKBAwfM43ft2tVsOS19H3z77bcCAOGdd94xj7v8b8mAAQOEixcvmscvXrxYACB8+OGHzZbj6uoqTJs2rdl4ueGhJRlQq9XXvHrJz88PAPDhhx/afaKem5sbJk+ebPX8iYmJ5r8YAGD8+PEIDg7G9u3b7Vq/tbZv3w4nJyc8+eSTFuPnzJkDQRCwY8cOi/GjRo1CeHi4+XGfPn3g4+ODP//887rr0Wq1FicVuri44Mknn0RdXR3279/fBmladuUerbNnz6K8vBxDhgyBIAj46aefms1/5dUal3fvX7x40byL+v3334evry/uvvtulJeXm4cBAwZArVa3eIjlMj8/PxQVFeHgwYNtlu/xxx+3eHznnXded3tY6x//+EezPWJOTk7m80SamppQWVmJhoYGDBw48LqHM65Vc0VFBWpqaq773LNnz6JTp07o1KkTIiIi8Mwzz2Dw4MHYunUrgEufvctX4DQ2NqKiosJ8eLil+pKSkmza63kjHPF58/Pzw3fffYfi4mKbaqmoqICzs3OrJ3Xfe++92L17d7Nh7ty5FvNVVlbis88+wwMPPIDa2lrz56GiogKjR4/GsWPHcPr0aQC2b5vWqNVqTJw40fxYr9fDz88PPXv2xKBBg8zjL///la/Xldv6r7/+QkVFBSIiIuDn59diDVOnTrXYwzpt2jQ4Ozu3+N3csWPHFvdiyQ0PLclAXV0dNBpNq9Pj4+Px5ptv4pFHHsHTTz+NqKgo3H///Rg/frzFJYrXEhISYtNJgT169LB4rFKpEBERccPnh1zPqVOnoNPpLJoo4P92o586dcpifJcuXZoto2PHjtc9N+TUqVPo0aNHs9evtfW0pYKCAjz//PP46KOPmtVpMpksHnfo0KHZibA33XQTAJi3xbFjx2AymVp9D5WVlbVay/z587Fnzx7cfvvtiIiIQHR0NBISEjB06FBbYwG4dDLt1Y2GNdvDWt26dWtx/Ntvv40lS5bg6NGj+Ouvv647/9Wufh917NgRwKVDBj4+Ptd8rru7O/7zn/8AuPQPY7du3dC5c2fz9KamJixbtgyrVq3CyZMnLc4Xaunwp7U1twVHfN4WL16MpKQkhIaGYsCAARgzZgwSExObvY9t1blz5xbPnykqKrJ4fPz4cQiCgOeeew7PPfdci8sqKytDSEiIzdvmWrVdfV6jr68vQkNDm40DYPF61dfXIzMzE+vWrcPp06ctzk26+vsAaP7drFarERwc3OJ3syAIsj/RF2AjI3lFRUUwmUyIiIhodR4PDw988cUX2LdvHz755BPs3LkTmzZtwl133YVPP/0UTk5O112PI/7Ca+0D0tjYaFVNbaG19QhXnagoFY2Njbj77rtRWVmJ+fPnIzIyEl5eXjh9+jSSk5Pt2uPW1NQEjUaD7OzsFqdf65yenj174vfff8fHH3+MnTt34oMPPsCqVavw/PPPIz093eZaHL3dW3ofv/vuu0hOTsZ9992HuXPnQqPRwMnJCZmZmeZzVa7nRt5HTk5O1zxBdeHChXjuuecwZcoUvPDCC/D390eHDh0wa9asFrd3e+2NsYc1r9MDDzyAO++8E1u3bsWnn36Kl19+GYsWLcKWLVsQExPT6rIDAgLQ0NCA2traZo2VLS6/pikpKRg9enSL81z+vrV127SmtdfFmtfrX//6F9atW4dZs2Zh8ODB8PX1hUqlwsSJE2/4Uvnq6moEBgbe0DKkgI2MxF3+/YzWPnCXdejQAVFRUYiKisLSpUuxcOFC/Pvf/8a+ffswatSoNu+6jx07ZvFYEAQcP37c4vduOnbsiOrq6mbPPXXqlMVfX7bUFhYWhj179jT7Mjt69Kh5elsICwvD4cOH0dTUZLFXpq3Xc7VffvkFf/zxB95++20kJiaax+/evbvF+ZuamvDnn3+a98IAwB9//AEA5qtPwsPDsWfPHgwdOtSufwS9vLwQHx+P+Ph4XLx4Effffz8WLFiA1NRUWVwGu3nzZnTv3h1btmyxeK9J5Xd/Nm/ejJEjR+Ktt96yGG/LPzK2fIak8HkLDg7GE088gSeeeAJlZWW49dZbsWDBgms2MpGRkQAuXb105feMrS5/97i4uFz3Cihrt40j92ps3rwZSUlJWLJkiXnc+fPnW/xuBS59N48cOdL8uK6uDiUlJRgzZozFfKdPn8bFixdbPClcbniOjIR99tlneOGFF9CtWzdMmjSp1fkqKyubjbt8JcblXw/18vICgFbf/LZ65513LM7b2bx5M0pKSiy+iMLDw3HgwAFcvHjRPO7jjz9udpm2LbWNGTMGjY2NeO211yzGv/rqq1CpVNf8IrTFmDFjUFpaik2bNpnHNTQ0YMWKFVCr1Rg+fHibrOdql/9Cu/IvMkEQsGzZslafc+VrIQgCXnvtNbi4uCAqKgrApb+AGxsb8cILLzR7bkNDwzVf94qKCovHrq6uuPnmmyEIgsUhGilr6TX97rvv8O2334pVkgUnJ6dme3bef/9983ka1rDlMyTm562xsbHZ4RCNRgOdTnfdXzoePHgwANzwL/RqNBqMGDECa9asQUlJSbPpZ86cMf+/tdumrb9fr9RSDStWrGj1JwveeOMNi8/m6tWr0dDQ0GxbHTp0CEDrV6zJCffISMSOHTtw9OhRNDQ0wGg04rPPPsPu3bsRFhaGjz766Jp/+WZkZOCLL77A2LFjERYWhrKyMqxatQqdO3fGsGHDAFxqKvz8/PD666/D29sbXl5eGDRokN3H2/39/TFs2DBMnjwZRqMRWVlZiIiIsLhE/JFHHsHmzZvx97//HQ888ABOnDiBd9991+JkQFtri42NxciRI/Hvf/8b+fn56Nu3Lz799FN8+OGHmDVrVrNl22vq1KlYs2YNkpOTcejQIXTt2hWbN2/G119/jaysrBvatX38+HG8+OKLzcb3798f0dHRCA8PR0pKCk6fPg0fHx988MEHrZ5D4u7ujp07dyIpKQmDBg3Cjh078Mknn+CZZ54xHzIaPnw4HnvsMWRmZiI3NxfR0dFwcXHBsWPH8P7772PZsmUYP358i8uPjo6GVqvF0KFDERQUhLy8PLz22msYO3bsDb0G7WncuHHYsmUL4uLiMHbsWJw8eRKvv/46br755hYvzRWjvoyMDEyePBlDhgzBL7/8guzsbJvOGenXrx+cnJywaNEimEwmuLm54a677mrxvCgxP2+1tbXo3Lkzxo8fj759+0KtVmPPnj04ePCgxR6HlnTv3h29e/fGnj17MGXKFJvWe7WVK1di2LBhuOWWW/Doo4+ie/fuMBqN+Pbbb1FUVGT+nRhrt01bf79eady4cdiwYQN8fX1x880349tvv8WePXtaPUfn4sWLiIqKwgMPPIDff/8dq1atwrBhw3DPPfdYzLd792506dJF/pdeA7z8WmyXL5m7PLi6ugparVa4++67hWXLlllc5nvZ1Zdf7927V7j33nsFnU4nuLq6CjqdTnjwwQeFP/74w+J5H374oXDzzTcLzs7OFpf4DR8+vNVLbFu7/Pp///d/hdTUVEGj0QgeHh7C2LFjLS4NvmzJkiVCSEiI4ObmJgwdOlT44Ycfmi3zWrVdfTmoIAhCbW2t8NRTTwk6nU5wcXERevToIbz88stCU1OTxXwAWrzMs7XLwq9mNBqFyZMnC4GBgYKrq6twyy23tHh5pa2XX6OVy0MffvhhQRAE4ciRI8KoUaMEtVotBAYGCo8++qj5MtarL2f28vISTpw4IURHRwuenp5CUFCQYDAYml0yKgiC8MYbbwgDBgwQPDw8BG9vb+GWW24R5s2bJxQXF5vnuXrbrFmzRvjb3/4mBAQECG5ubkJ4eLgwd+5cwWQyXTNna5dfe3l5NZv36vezNVq7/Prll19uNm9TU5OwcOFCISwsTHBzcxP69+8vfPzxxy2+t9DK5ddX/7TB5c/tyZMnr1lna5mvdP78eWHOnDlCcHCw4OHhIQwdOlT49ttvW/3svf/++y0uZ+3atUL37t0FJycni0uxpfR5u3DhgjB37lyhb9++gre3t+Dl5SX07dtXWLVq1TVfo8uWLl0qqNXqZpckt7ZuQWj9Jy5OnDghJCYmClqtVnBxcRFCQkKEcePGCZs3bzbPY+22EQTbv19b+964OktVVZX5e0itVgujR48Wjh492ux77HLO/fv3C1OnThU6duwoqNVqYdKkSUJFRYXFOhobG4Xg4GDh2WefbfE1kxuVIEj0rEciIqIrmEwmdO/eHYsXL8bDDz8sdjmScvkHLw8ePGhxu5uWbNu2DQkJCThx4gSCg4PbqULH4TkyREQkC76+vpg3bx5efvllSd/cUuoWLVqEGTNmKKKJAXiODBERycj8+fMxf/58scuQNamc6N5WuEeGiIiIZIvnyBAREZFscY8MERERyRYbGSIiIpItxZ/s29TUhOLiYnh7eyvi5lhERET/DQRBQG1tLXQ63TVvgKz4Rqa4uLjZHUaJiIhIHgoLCy3uGH81xTcyl39Gfd132+Gp9hK5GiIiovYRdEbe1/KcPXvWqtuhKL6RuXw4yVPtBU9vtcjVEBERtQ91vbwbmcuud1oIT/YlIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhkS/L3Wvriiy/w8ssv49ChQygpKcHWrVtx3333iV2WWfHJArw624Caymp4eqsxa0kawvThYpdlMyXkUEIGQBk5lJABYA4pUUIGQNo5YmNj4eLiAnd3dwBAcnIyoqOjUVBQgLS0NFRXV0OtVsNgMCA8/FLN9k5rS5LfI3P27Fn07dsXK1euFLuUFq1MXYDRCXFYs38rxk9LQtacNLFLsosScighA6CMHErIADCHlCghAyD9HJmZmcjJyUFOTg6io6MBAAsXLkRcXBy2bNmCxMREpKenm+e3d1pbknwjExMTgxdffBFxcXFil9JMdXkljh3Ow8i4MQCAIWOiUF5iRHF+ociV2UYJOZSQAVBGDiVkAJhDSpSQAZBnjsrKSuTl5SEmJgYAEBUVBaPRiMLCQruntTXJNzJSVl5shL8mEE7Ol47QqVQqdNJpceZ0iciV2UYJOZSQAVBGDiVkAJhDSpSQAZBHDoPBgPj4eGRkZKCqqgpGoxEBAQFwvqLmoKAglJaW2j2trSmukblw4QJqamosBiIiIrq2tWvXYuPGjcjOzoafnx8MBoPYJVlFcY1MZmYmfH19zUNoaKjD1hWoC0JlWTkaGxoAAIIg4ExxKTqFBDtsnY6ghBxKyAAoI4cSMgDMISVKyABIP4dWqwUAODs7IyEhAbm5uQgKCkJFRQUarqjZaDRCq9XaPa2tKa6RSU1NhclkMg+OOB53mV+gP8J7R2Lf1u0AgG+270WgVgNdV8c1T46ghBxKyAAoI4cSMgDMISVKyABIO0d9fT1qa2vNj3fu3Am9Xg9/f3/o9Xrs2LEDALB3715oNBqEhobaPa2tqQRBENp8qQ6iUqlsvvy6pqYGvr6+2PTbfnh6q9u8pqIT+ciak4baKhM81V6YucSArpE92nw9jqaEHErIACgjhxIyAMwhJUrIALRvjuAy6/95Lyoqwrx589DU1ARBEBASEoKUlBTodDrk5+cjPT0dJpMJXl5eMBgMiIiIAAC7p1mjrq4OI0aMgMlkgo+PT6vzSb6Rqaurw/HjxwEA/fv3x9KlSzFy5Ej4+/ujS5cu132+oxsZIiIiKbKlkZEiaxsZyf8g3g8//ICRI0eaH8+ePRsAkJSUhPXr14tUFREREUmB5BuZESNGQOI7jYiIiEgkijvZl4iIiP57sJEhIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhkS/I3jSQiIvkILpP/TX5LNCqxS2gTcs9xzsO6+rlHhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhItiR99+vMzExs2bIFR48ehYeHB4YMGYJFixZBr9eLXZpZ8ckCvDrbgJrKanh6qzFrSRrC9OFil2UzJeRQQgZAGTmUkAFgjvYQGxsLFxcXuLu7AwCSk5MRHR2NgoICpKWlobq6Gmq1GgaDAeHhl2q2d5oUSHlbWEtqGSS9R2b//v2YPn06Dhw4gN27d+Ovv/5CdHQ0zp49K3ZpZitTF2B0QhzW7N+K8dOSkDUnTeyS7KKEHErIACgjhxIyAMzRXjIzM5GTk4OcnBxER0cDABYuXIi4uDhs2bIFiYmJSE9PN89v7zQpkPq2sIbUMki6kdm5cyeSk5PRq1cv9O3bF+vXr0dBQQEOHTokdmkAgOryShw7nIeRcWMAAEPGRKG8xIji/EKRK7ONEnIoIQOgjBxKyAAwh5gqKyuRl5eHmJgYAEBUVBSMRiMKCwvtniYFctwWV5NiBkk3MlczmUwAAH9/f5EruaS82Ah/TSCcnC8doVOpVOik0+LM6RKRK7ONEnIoIQOgjBxKyAAwR3syGAyIj49HRkYGqqqqYDQaERAQAOcrag4KCkJpaand06RADtvieqSYQTaNTFNTE2bNmoWhQ4eid+/erc534cIF1NTUWAxERCRNa9euxcaNG5GdnQ0/Pz8YDAaxSyKZkU0jM336dPz666/YuHHjNefLzMyEr6+veQgNDXVYTYG6IFSWlaOxoQEAIAgCzhSXolNIsMPW6QhKyKGEDIAycighA8Ac7UWr1QIAnJ2dkZCQgNzcXAQFBaGiogINV9RsNBqh1WrtniYFUt8W1pBiBlk0MjNmzMDHH3+Mffv2oXPnztecNzU1FSaTyTw48tioX6A/wntHYt/W7QCAb7bvRaBWA11XxzVPjqCEHErIACgjhxIyAMzRHurr61FbW2t+vHPnTuj1evj7+0Ov12PHjh0AgL1790Kj0SA0NNTuaVIg5W1hLSlmUAmCIIi29usQBAH/+te/sHXrVnz++efo0aOHzcuoqamBr68vNv22H57e6javsehEPrLmpKG2ygRPtRdmLjGga6TtdYpNCTmUkAFQRg4lZACYwx7BZdb/k1JUVIR58+ahqakJgiAgJCQEKSkp0Ol0yM/PR3p6OkwmE7y8vGAwGBAREQEAdk+zVolGZdP8tlDCe6q9MpyrrUN8r+EwmUzw8fFpdT5JNzJPPPEEcnJy8OGHH1r8doyvry88PDysWoajGxkiIvo/tjQyUuXIRoasZ20jI+lDS6tXr4bJZMKIESMQHBxsHjZt2iR2aURERCQBkv5lXwnvLCIiIiIJkPQeGSIiIqJrYSNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJlqRvGklEjhVcJv8bs5ZoVGKXQFfg9qD2xj0yREREJFtsZIiIiEi22MgQERGRbLGRISIiItliI0NERESyxUaGiIiIZIuNDBEREckWGxkiIiKSLTYyREREJFtsZIiIiEi22MgQERGRbLGRISIiItliI0NERESyJem7X69evRqrV69Gfn4+AKBXr154/vnnERMTI25hVyg+WYBXZxtQU1kNT281Zi1JQ5g+XOyybKaEHErIAEg7R2xsLFxcXODu7g4ASE5ORnR0NAoKCpCWlobq6mqo1WoYDAaEh1+q2d5pUiDlbWELJeRQQgZAGTmklkHSe2Q6d+6Ml156CYcOHcIPP/yAu+66C/feey9+++03sUszW5m6AKMT4rBm/1aMn5aErDlpYpdkFyXkUEIGQPo5MjMzkZOTg5ycHERHRwMAFi5ciLi4OGzZsgWJiYlIT083z2/vNCmQ+rawlhJyKCEDoIwcUssg6UYmNjYWY8aMQY8ePXDTTTdhwYIFUKvVOHDggNilAQCqyytx7HAeRsaNAQAMGROF8hIjivMLRa7MNkrIoYQMgDxzVFZWIi8vz7ynNCoqCkajEYWFhXZPkwI5bouWKCGHEjIAysghxQySbmSu1NjYiI0bN+Ls2bMYPHiw2OUAAMqLjfDXBMLJ+dIROpVKhU46Lc6cLhG5MtsoIYcSMgDyyGEwGBAfH4+MjAxUVVXBaDQiICAAzlfUHBQUhNLSUrunSYEctoU1lJBDCRkAZeSQYgZJnyMDAL/88gsGDx6M8+fPQ61WY+vWrbj55ptbnf/ChQu4cOGC+XFNTU17lEn0X2Ht2rXQarVoaGjAqlWrYDAYMG3aNLHLIqL/YpLfI6PX65Gbm4vvvvsO06ZNQ1JSEo4cOdLq/JmZmfD19TUPoaGhDqstUBeEyrJyNDY0AAAEQcCZ4lJ0Cgl22DodQQk5lJABkH4OrVYLAHB2dkZCQgJyc3MRFBSEiooKNFxRs9FohFartXuaFEh9W1hLCTmUkAFQRg4pZpB8I+Pq6oqIiAgMGDAAmZmZ6Nu3L5YtW9bq/KmpqTCZTObBkcfb/QL9Ed47Evu2bgcAfLN9LwK1Gui6Oq55cgQl5FBCBkDaOerr61FbW2t+vHPnTuj1evj7+0Ov12PHjh0AgL1790Kj0SA0NNTuaVIg5W1hCyXkUEIGQBk5pJhBJQiCINra7XDXXXehS5cuWL9+vVXz19TUwNfXF5t+2w9Pb3Wb11N0Ih9Zc9JQW2WCp9oLM5cY0DWyR5uvx9GUkEMJGYD2zRFcZv3Hv6ioCPPmzUNTUxMEQUBISAhSUlKg0+mQn5+P9PR0mEwmeHl5wWAwICIiAgDsnmatEo3KpvltwfeUdCghA6CMHO2V4VxtHeJ7DYfJZIKPj0+r80m6kUlNTUVMTAy6dOmC2tpa5OTkYNGiRdi1axfuvvtuq5bh6EaGSM5saWSkypGNDBGJx9pGRtIn+5aVlSExMRElJSXw9fVFnz59bGpiiIiISNkk3ci89dZbYpdAREREEib5k32JiIiIWsNGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki1J3zSSiByrRKMSu4QbFlwmiF1Cm1DCtiASA/fIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJlqwamZdeegkqlQqzZs0SuxSz4pMFmBs3GY8Nj8NT4/6JU7+fELskuyghhxIyAMrIIfUMsbGxuP/++5GQkICEhAR8+umnAICCggJMmTIF999/PxITE3HixP/Vbe80KZD69rCGEjIAysghtQyyaWQOHjyINWvWoE+fPmKXYmFl6gKMTojDmv1bMX5aErLmpIldkl2UkEMJGQBl5JBDhszMTOTk5CAnJwfR0dEAgIULFyIuLg5btmxBYmIi0tPTzfPbO00K5LA9rkcJGQBl5JBaBlk0MnV1dZg0aRLWrl2Ljh07il2OWXV5JY4dzsPIuDEAgCFjolBeYkRxfqHIldlGCTmUkAFQRg65ZqisrEReXh5iYmIAAFFRUTAajSgsLLR7mhTIdXtcSQkZAGXkkGIGWTQy06dPx9ixYzFq1KjrznvhwgXU1NRYDI5SXmyEvyYQTs7OAACVSoVOOi3OnC5x2DodQQk5lJABUEYOuWQwGAyIj49HRkYGqqqqYDQaERAQAOcr6g4KCkJpaand06RALtvjWpSQAVBGDilmkHwjs3HjRvz444/IzMy0av7MzEz4+vqah9DQUAdXSERys3btWmzcuBHZ2dnw8/ODwWAQuyQispOkG5nCwkLMnDkT2dnZcHd3t+o5qampMJlM5sGRu3cDdUGoLCtHY0MDAEAQBJwpLkWnkGCHrdMRlJBDCRkAZeSQQwatVgsAcHZ2RkJCAnJzcxEUFISKigo0XFG30WiEVqu1e5oUyGF7XI8SMgDKyCHFDJJuZA4dOoSysjLceuutcHZ2hrOzM/bv34/ly5fD2dkZjY2NzZ7j5uYGHx8fi8FR/AL9Ed47Evu2bgcAfLN9LwK1Gui6ymsvkBJyKCEDoIwcUs9QX1+P2tpa8+OdO3dCr9fD398fer0eO3bsAADs3bsXGo0GoaGhdk+TAqlvD2soIQOgjBxSzKASBEEQbe3XUVtbi1OnTlmMmzx5MiIjIzF//nz07t37usuoqamBr68vNv22H57e6javsehEPrLmpKG2ygRPtRdmLjGga2SPNl+PoykhhxIyAMrI0Z4Zgsts+worKirCvHnz0NTUBEEQEBISgpSUFOh0OuTn5yM9PR0mkwleXl4wGAyIiIgAALunWatEo7JpflvwPSUdSsjRXhnO1dYhvtdwmEyma+6UkHQj05IRI0agX79+yMrKsmp+RzcyRCQuWxsZqXJkI0MkR9Y2MpI+tERERER0Lc5iF2Crzz//XOwSiIiISCK4R4aIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW7K7aSSRFASXCWKX0CZKNCqxS7hhSshARPbjHhkiIiKSLTYyREREJFtsZIiIiEi22MgQERGRbLGRISIiItliI0NERESyxUaGiIiIZIuNDBEREckWGxkiIiKSLTYyREREJFtsZIiIiEi27GpkjEYj/vnPf0Kn08HZ2RlOTk4WAxEREVF7sOumkcnJySgoKMBzzz2H4OBgqFS8aRsRERG1P7sama+++gpffvkl+vXr18blWEpLS0N6errFOL1ej6NHjzp0vbYoPlmAV2cbUFNZDU9vNWYtSUOYPlzssmymhBxSzhAbGwsXFxe4u7sDuPTHQHR0NAoKCpCWlobq6mqo1WoYDAaEh1+q2d5pUiDlbWEL5pAOJWQAlJFDahnsOrQUGhoKQRDaupYW9erVCyUlJebhq6++apf1Wmtl6gKMTojDmv1bMX5aErLmpIldkl2UkEPqGTIzM5GTk4OcnBxER0cDABYuXIi4uDhs2bIFiYmJFo27vdOkQOrbwlrMIR1KyAAoI4fUMtjVyGRlZeHpp59Gfn5+G5fTnLOzM7RarXkIDAx0+DqtVV1eiWOH8zAybgwAYMiYKJSXGFGcXyhyZbZRQg45ZqisrEReXh5iYmIAAFFRUTAajSgsLLR7mhTIcVu0hDmkQwkZAGXkkGIGuxqZ+Ph4fP755wgPD4e3tzf8/f0thrZ07Ngx6HQ6dO/eHZMmTUJBQcE1579w4QJqamosBkcpLzbCXxMIJ+dLR+hUKhU66bQ4c7rEYet0BCXkkEMGg8GA+Ph4ZGRkoKqqCkajEQEBAXC+ouagoCCUlpbaPU0K5LAtrMEc0qGEDIAyckgxg13nyGRlZbVxGS0bNGgQ1q9fD71ej5KSEqSnp+POO+/Er7/+Cm9v7xafk5mZKbnd7ERr166FVqtFQ0MDVq1aBYPBgGnTpoldFhGR7NnVyCQlJbV1HS26vOscAPr06YNBgwYhLCwM7733Hh5++OEWn5OamorZs2ebH9fU1CA0NNQh9QXqglBZVo7GhgY4OTtDEAScKS5Fp5Bgh6zPUZSQQ+oZtFotgEuHShMSEnD//fcjKCgIFRUVaGhogPP/r9loNEKr1cLLy8uuaVIg9W1hLeaQDiVkAJSRQ4oZ7P5BvMbGRnzwwQd48cUX8eKLL2Lr1q1obGxsy9qa8fPzw0033YTjx4+3Oo+bmxt8fHwsBofVE+iP8N6R2Ld1OwDgm+17EajVQNfVMY2Toyghh5Qz1NfXo7a21vx4586d0Ov18Pf3h16vx44dOwAAe/fuhUajQWhoqN3TpEDK28IWzCEdSsgAKCOHFDOoBDsuPzp+/DjGjBmD06dPQ6/XAwB+//13hIaG4pNPPnHYZaB1dXXo0qUL0tLS8OSTT1r1nJqaGvj6+mLTb/vh6a1u85qKTuQja04aaqtM8FR7YeYSA7pG9mjz9TiaEnK0Z4bgMus/NkVFRZg3bx6ampogCAJCQkKQkpICnU6H/Px8pKenw2QywcvLCwaDAREREQBg9zRblGgc8xtQSng/AcwhJUrIACgjR3tlOFdbh/hew2Eyma65U8KuRmbMmDEQBAHZ2dnmk3srKirw0EMPoUOHDvjkk0/sr/wKKSkpiI2NRVhYGIqLi2EwGJCbm4sjR46gU6dOVi3D0Y0M/XeypZGRMkc1MkREN8raRsauc2T279+PAwcOWFyhFBAQgJdeeglDhw61Z5EtKioqwoMPPoiKigp06tQJw4YNw4EDB6xuYoiIiEjZ7Gpk3NzcLI75X1ZXVwdXV9cbLuqyjRs3ttmyiIiISHnsOtl33LhxmDp1Kr777jsIggBBEHDgwAE8/vjjuOeee9q6RiIiIqIW2dXILF++HOHh4Rg8eDDc3d3h7u6OoUOHIiIiAsuWLWvrGomIiIhaZNehJT8/P3z44Yc4duyY+QaOPXv2tOuqCSIiIiJ72dXIXNajRw/06CGvy8aIiIhIOaxuZGbPno0XXngBXl5eFr+c25KlS5fecGFERERE12N1I/PTTz/hr7/+Mv8/ERERkdisbmT27dvX4v8TERERicWuq5amTJnS4u/InD17FlOmTLnhooiIiIisYVcj8/bbb6O+vr7Z+Pr6erzzzjs3XBQRERGRNWy6aqmmpsb8A3i1tbVwd3c3T2tsbMT27duh0WjavEgiIiKiltjUyPj5+UGlUkGlUuGmm25qNl2lUiE9Pb3NiiMiIiK6FpsamX379kEQBNx111344IMPLG4a6erqirCwMOh0ujYvkoiIiKglNjUyw4cPBwCcPHkSXbp0gUqlckhRRFJXouF7n0ipgssEsUtoE/8t31NWNzKHDx9G79690aFDB5hMJvzyyy+tztunT582KY6IiIjoWqxuZPr164fS0lJoNBr069cPKpUKgtC8a1WpVGhsbGzTIomIiIhaYnUjc/LkSXTq1Mn8/0RERERis7qRCQsLM///qVOnMGTIEDg7Wz69oaEB33zzjcW8RERERI5i1w/ijRw5EpWVlc3Gm0wmjBw58oaLIiIiIrKGXY2MIAgtXrFUUVEBLy+vGy6KiIiIyBo2XX59//33A7h0Qm9ycjLc3NzM0xobG3H48GEMGTKkbSskIiIiaoVNjYyvry+AS3tkvL294eHhYZ7m6uqKO+64A48++mjbVkhERETUCpsamXXr1pkvuV6xYgXUarVDiiIiIiKyhs3nyAiCgOzsbJSUlDiiHiIiIiKr2dzIdOjQAT169EBFRYUj6iEiIiKyml1XLb300kuYO3cufv3117auh4iIiMhqNp0jc1liYiLOnTuHvn37wtXV1eKkXwAt/sYMERERUVuzq5HJyspq4zJad/r0acyfPx87duzAuXPnEBERgXXr1mHgwIHtVsO1FJ8swKuzDaiprIantxqzlqQhTB8udlk2U0IOJWQAlJFDCRkA5pASKWeIjY2Fi4sL3N3dAQDJycmIjo5GQUEB0tLSUF1dDbVaDYPBgPDwSzXbO00KpLYtVEJLd36UiKqqKvTv3x8jR47EtGnT0KlTJxw7dgzh4eFWb9Samhr4+vpi02/74end9ldZ/XviYxj5j7EYNeEefP3JHmxe/TZe/XhDm6/H0ZSQQwkZAGXkUEIGgDmkpD0zBJfZ9s9ibGwsXnnlFej1eovxjz/+OMaOHYvY2Fjs2bMH77zzDt55550bmmaLEk3zH65tC+21Lc7V1iG+13CYTCb4+Pi0Op9d58hc6fz586ipqbEY2sqiRYsQGhqKdevW4fbbb0e3bt0QHR0tmc60urwSxw7nYWTcGADAkDFRKC8xoji/UOTKbKOEHErIACgjhxIyAMwhJXLMUFlZiby8PMTExAAAoqKiYDQaUVhYaPc0KZDitrCrkTl79ixmzJgBjUYDLy8vdOzY0WJoKx999BEGDhyICRMmQKPRoH///li7du01n3PhwgWHNVZXKy82wl8TCKf/f/NMlUqFTjotzpyW16XpSsihhAyAMnIoIQPAHFIihwwGgwHx8fHIyMhAVVUVjEYjAgICzDdXVqlUCAoKQmlpqd3TpECK28KuRmbevHn47LPPsHr1ari5ueHNN99Eeno6dDqdXbu/WvPnn39i9erV6NGjB3bt2oVp06bhySefxNtvv93qczIzM+Hr62seQkND26weIiKiq61duxYbN25EdnY2/Pz8YDAYxC7pv4pdjcx//vMfrFq1Cv/4xz/g7OyMO++8E88++ywWLlyI7OzsNiuuqakJt956KxYuXIj+/ftj6tSpePTRR/H666+3+pzU1FSYTCbz4MjdcYG6IFSWlaOxoQHApR8LPFNcik4hwQ5bpyMoIYcSMgDKyKGEDABzSInUM2i1WgCAs7MzEhISkJubi6CgIFRUVKDhipqNRiO0Wq3d06RAitvCrkamsrIS3bt3BwD4+PiYL7ceNmwYvvjiizYrLjg4GDfffLPFuJ49e6KgoKDV57i5ucHHx8dicBS/QH+E947Evq3bAQDfbN+LQK0Guq7y2gukhBxKyAAoI4cSMgDMISVSzlBfX4/a2lrz4507d0Kv18Pf3x96vR47duwAAOzduxcajQahoaF2T5MCKW4Lu65a6tOnD1asWIHhw4dj1KhR6NevH1555RUsX74cixcvRlFRUZsUl5CQgMLCQnz55ZfmcU899RS+++47fPPNN1Ytw9FXLRWdyEfWnDTUVpngqfbCzCUGdI3s0ebrcTQl5FBCBkAZOZSQAWAOKWnPDLZctVRUVIR58+ahqakJgiAgJCQEKSkp0Ol0yM/PR3p6OkwmE7y8vGAwGBAREQEAdk+zhaOuWmqvbWHtVUt2NTKvvvoqnJyc8OSTT2LPnj2IjY2FIAj466+/sHTpUsycOfOGir/s4MGDGDJkCNLT0/HAAw/g+++/x6OPPoo33ngDkyZNsmoZjm5kiIhIWWy9/FqqHNXItBdrGxmbfhCvqakJL7/8Mj766CNcvHgRxcXFMBgMOHr0KA4dOoSIiAj06dPnhou/7LbbbsPWrVuRmpqKjIwMdOvWDVlZWVY3MURERKRsNjUyCxYsQFpaGkaNGgUPDw8sW7YMZWVl+J//+R+EhYU5pMBx48Zh3LhxDlk2ERERyZtNJ/u+8847WLVqFXbt2oVt27bhP//5D7Kzs9HU1OSo+oiIiIhaZVMjU1BQgDFjxpgfjxo1CiqVCsXFxW1eGBEREdH12NTINDQ0mG+KdZmLiwv++uuvNi2KiIiIyBo2nSMjCAKSk5Ph5uZmHnf+/Hk8/vjj8PLyMo/bsmVL21VIRERE1AqbGpmkpKRm4x566KE2K4aIiIjIFjY1MuvWrXNUHUREREQ2s+sWBURERERSwEaGiIiIZIuNDBEREckWGxkiIiKSLTYyREREJFtsZIiIiEi2bLr8msSlhFvLy/228kSkfPyekhfukSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki3J3/26a9euOHXqVLPxTzzxBFauXClCRZaKTxbg1dkG1FRWw9NbjVlL0hCmDxe7LABAbGwsXFxc4O7uDgBITk5GdHQ0CgoKkJaWhurqaqjVahgMBoSHX6rZ3mlSIOVtYQsl5FBCBoA5pEQJGQBl5JBaBsnvkTl48CBKSkrMw+7duwEAEyZMELmyS1amLsDohDis2b8V46clIWtOmtglWcjMzEROTg5ycnIQHR0NAFi4cCHi4uKwZcsWJCYmIj093Ty/vdOkQOrbwlpKyKGEDABzSIkSMgDKyCG1DJJvZDp16gStVmsePv74Y4SHh2P48OFil4bq8kocO5yHkXFjAABDxkShvMSI4vxCkStrXWVlJfLy8hATEwMAiIqKgtFoRGFhod3TpECO26IlSsihhAwAc0iJEjIAysghxQySb2SudPHiRbz77ruYMmUKVCpVi/NcuHABNTU1FoOjlBcb4a8JhJPzpSN0KpUKnXRanDld4rB12spgMCA+Ph4ZGRmoqqqC0WhEQEAAnK+oOSgoCKWlpXZPkwI5bAtrKCGHEjIAzCElSsgAKCOHFDPIqpHZtm0bqqurkZyc3Oo8mZmZ8PX1NQ+hoaHtV6DErF27Fhs3bkR2djb8/PxgMBjELomIiKhNyaqReeuttxATEwOdTtfqPKmpqTCZTObBkYc+AnVBqCwrR2NDAwBAEAScKS5Fp5Bgh63TFlqtFgDg7OyMhIQE5ObmIigoCBUVFWi4omaj0QitVmv3NCmQ+rawlhJyKCEDwBxSooQMgDJySDGDbBqZU6dOYc+ePXjkkUeuOZ+bmxt8fHwsBkfxC/RHeO9I7Nu6HQDwzfa9CNRqoOsq/l6g+vp61NbWmh/v3LkTer0e/v7+0Ov12LFjBwBg79690Gg0CA0NtXuaFEh5W9hCCTmUkAFgDilRQgZAGTmkmEElCIIg2tptkJaWhjVr1qCwsNB8noY1ampq4Ovri02/7Yent7rN6yo6kY+sOWmorTLBU+2FmUsM6BrZo83XAwDBZdZvqqKiIsybNw9NTU0QBAEhISFISUmBTqdDfn4+0tPTYTKZ4OXlBYPBgIiICACwe5q1SjQtn9vUFtpzWziSEnIoIQPAHFKihAyAMnK0V4ZztXWI7zUcJpPpmjslZNHINDU1oVu3bnjwwQfx0ksv2fRcRzcy7cmWRkaqHNnIEBGRcljbyMji0NKePXtQUFCAKVOmiF0KERERSYjkf9kXAKKjoyGDHUdERETUzmSxR4aIiIioJWxkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2ZLFTSPbQtAZAep6ed94skSjErsEIiIiSeEeGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2ZL03a8bGxuRlpaGd999F6WlpdDpdEhOTsazzz4Llcqxd4KOjY2Fi4sL3N3dAQDJycmIjo5GQUEB0tLSUF1dDbVaDYPBgPDwcACwe5oUFJ8swKuzDaiprIantxqzlqQhTC+d+qyhhAyAMnIoIQPAHFKihAyAMnJILYOk98gsWrQIq1evxmuvvYa8vDwsWrQIixcvxooVK9pl/ZmZmcjJyUFOTg6io6MBAAsXLkRcXBy2bNmCxMREpKenm+e3d5oUrExdgNEJcVizfyvGT0tC1pw0sUuymRIyAMrIoYQMAHNIiRIyAMrIIbUMkm5kvvnmG9x7770YO3YsunbtivHjxyM6Ohrff/+9KPVUVlYiLy8PMTExAICoqCgYjUYUFhbaPU0KqssrcexwHkbGjQEADBkThfISI4rzpVGfNZSQAVBGDiVkAJhDSpSQAVBGDilmkHQjM2TIEOzduxd//PEHAODnn3/GV199ZW4IWnLhwgXU1NRYDPYyGAyIj49HRkYGqqqqYDQaERAQAGfnS0fkVCoVgoKCUFpaavc0KSgvNsJfEwinK+rrpNPizOkSkSuznhIyAMrIoYQMAHNIiRIyAMrIIcUMkj5H5umnn0ZNTQ0iIyPh5OSExsZGLFiwAJMmTWr1OZmZmW1y2Gbt2rXQarVoaGjAqlWrYDAYMG3atBteLhEREbUdSe+Ree+995CdnY2cnBz8+OOPePvtt/HKK6/g7bffbvU5qampMJlM5sHewzdarRYA4OzsjISEBOTm5iIoKAgVFRVoaGgAAAiCAKPRCK1Wa/c0KQjUBaGyrByNV9R3prgUnUKCRa7MekrIACgjhxIyAMwhJUrIACgjhxQzSLqRmTt3Lp5++mlMnDgRt9xyC/75z3/iqaeeQmZmZqvPcXNzg4+Pj8Vgq/r6etTW1pof79y5E3q9Hv7+/tDr9dixYwcAYO/evdBoNAgNDbV7mhT4BfojvHck9m3dDgD4ZvteBGo10HWVRn3WUEIGQBk5lJABYA4pUUIGQBk5pJhBJQiCINraryMgIAAvvviixSGdzMxMrFu3znzezPXU1NTA19cXn3/+OdRqtVXPKSoqwrx589DU1ARBEBASEoKUlBTodDrk5+cjPT0dJpMJXl5eMBgMiIiIAAC7p1mrROO4S86LTuQja04aaqtM8FR7YeYSA7pG9nDY+hxBCRkAZeRQQgaAOaRECRkAZeRorwznausQ32s4TCbTNXdKSLqRSU5Oxp49e7BmzRr06tULP/30E6ZOnYopU6Zg0aJFVi3DnkZGqhzZyBAREUmJtY2MpE/2XbFiBZ577jk88cQTKCsrg06nw2OPPYbnn39e7NKIiIhIAiS9R6YtcI8MERGR/Fi7R0bSJ/sSERERXQsbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLYkfffrtmTspEKtN2+6SETSFFymjPv38ua21N64R4aIiIhki40MERERyRYbGSIiIpItNjJEREQkW2xkiIiISLbYyBAREZFssZEhIiIi2WIjQ0RERLLFRoaIiIhki40MERERyRYbGSIiIpItNjJEREQkW/81N410lOKTBXh1tgE1ldXw9FZj1pI0hOnDxS7LZkrIoYQMgDJyKCEDIO0csbGxcHFxgbu7OwAgOTkZ0dHRKCgoQFpaGqqrq6FWq2EwGBAefqlme6dJgZS3hS2UkENqGSS/R6a2thazZs1CWFgYPDw8MGTIEBw8eFDsssxWpi7A6IQ4rNm/FeOnJSFrTprYJdlFCTmUkAFQRg4lZACknyMzMxM5OTnIyclBdHQ0AGDhwoWIi4vDli1bkJiYiPT0dPP89k6TAqlvC2spIYfUMki+kXnkkUewe/dubNiwAb/88guio6MxatQonD59WuzSUF1eiWOH8zAybgwAYMiYKJSXGFGcXyhyZbZRQg4lZACUkUMJGQB55qisrEReXh5iYmIAAFFRUTAajSgsLLR7mhTIcVu0RAk5pJhB0o1MfX09PvjgAyxevBh/+9vfEBERgbS0NERERGD16tVil4fyYiP8NYFwcr50hE6lUqGTToszp0tErsw2SsihhAyAMnIoIQMgjxwGgwHx8fHIyMhAVVUVjEYjAgIC4HxFzUFBQSgtLbV7mhTIYVtYQwk5pJhB0o1MQ0MDGhsbzceAL/Pw8MBXX33V4nMuXLiAmpoai4GISGnWrl2LjRs3Ijs7G35+fjAYDGKXRCQKSTcy3t7eGDx4MF544QUUFxejsbER7777Lr799luUlLTc/WVmZsLX19c8hIaGOqy+QF0QKsvK0djQAAAQBAFnikvRKSTYYet0BCXkUEIGQBk5lJABkH4OrVYLAHB2dkZCQgJyc3MRFBSEiooKNFxRs9FohFartXuaFEh9W1hLCTmkmEHSjQwAbNiwAYIgICQkBG5ubli+fDkefPBBdOjQcumpqakwmUzmwZHHeP0C/RHeOxL7tm4HAHyzfS8CtRroujqueXIEJeRQQgZAGTmUkAGQdo76+nrU1taaH+/cuRN6vR7+/v7Q6/XYsWMHAGDv3r3QaDQIDQ21e5oUSHlb2EIJOaSYQSUIgiDa2m1w9uxZ1NTUIDg4GPHx8airq8Mnn3xy3efV1NTA19cXm37bD09vdZvXVXQiH1lz0lBbZYKn2gszlxjQNbJHm6/H0ZSQQwkZAGXkUEIGoH1zBJdZ/1VcVFSEefPmoampyfyHXkpKCnQ6HfLz85Geng6TyQQvLy8YDAZEREQAgN3TbFGiUdn8HGvwPSUd7ZXhXG0d4nsNh8lkgo+PT6vzyaaRuayqqgrdunXD4sWLMXXq1OvO7+hGhoioLdjSyEiZoxoZ+u9jbSMj+R/E27VrFwRBgF6vx/HjxzF37lxERkZi8uTJYpdGREREIpP8OTImkwnTp09HZGQkEhMTMWzYMOzatQsuLi5il0ZEREQik/wemQceeAAPPPCA2GUQERGRBEl+jwwRERFRa9jIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREsiX5m0aSsgSXCWKX0CZKNCqxSyCF4XuKyD7cI0NERESyxUaGiIiIZIuNDBEREckWGxkiIiKSLTYyREREJFtsZIiIiEi22MgQERGRbLGRISIiItliI0NERESyxUaGiIiIZIuNDBEREckWGxkiIiKSLd408gYVnyzAq7MNqKmshqe3GrOWpCFMHy52WTaTco7Y2Fi4uLjA3d0dAJCcnIzo6GgUFBQgLS0N1dXVUKvVMBgMCA+/VLO906RAytvCWkrIADCHlCghA6CMHFLLIOoemS+++AKxsbHQ6XRQqVTYtm2bxXRBEPD8888jODgYHh4eGDVqFI4dOyZOsa1YmboAoxPisGb/VoyfloSsOWlil2QXqefIzMxETk4OcnJyEB0dDQBYuHAh4uLisGXLFiQmJiI9Pd08v73TpEDq28IaSsgAMIeUKCEDoIwcUssgaiNz9uxZ9O3bFytXrmxx+uLFi7F8+XK8/vrr+O677+Dl5YXRo0fj/Pnz7Vxpy6rLK3HscB5Gxo0BAAwZE4XyEiOK8wtFrsw2csxRWVmJvLw8xMTEAACioqJgNBpRWFho9zQpkOO2uJoSMgDMISVKyAAoI4cUM4jayMTExODFF19EXFxcs2mCICArKwvPPvss7r33XvTp0wfvvPMOiouLm+25EUt5sRH+mkA4OV86QqdSqdBJp8WZ0yUiV2YbOeQwGAyIj49HRkYGqqqqYDQaERAQAOcrag4KCkJpaand06RADtviepSQAWAOKVFCBkAZOaSYQbIn+548eRKlpaUYNWqUeZyvry8GDRqEb7/9ttXnXbhwATU1NRYDydvatWuxceNGZGdnw8/PDwaDQeySiIhIIiTbyFz+CzkoKMhi/PX+es7MzISvr695CA0NdViNgbogVJaVo7GhAcClvUhnikvRKSTYYet0BKnn0Gq1AABnZ2ckJCQgNzcXQUFBqKioQMMVNRuNRmi1WrunSYHUt4U1lJABYA4pUUIGQBk5pJhBso2MvVJTU2EymcyDI8998Av0R3jvSOzbuh0A8M32vQjUaqDr6rjmyRGknKO+vh61tbXmxzt37oRer4e/vz/0ej127NgBANi7dy80Gg1CQ0PtniYFUt4W1lJCBoA5pEQJGQBl5JBiBpUgCIJoa7+CSqXC1q1bcd999wEA/vzzT4SHh+Onn35Cv379zPMNHz4c/fr1w7Jly6xabk1NDXx9fbHpt/3w9Fa3ed1FJ/KRNScNtVUmeKq9MHOJAV0je7T5ehytvXIEl9n2disqKsK8efPQ1NQEQRAQEhKClJQU6HQ65OfnIz09HSaTCV5eXjAYDIiIiAAAu6dZq0Sjsml+WyjhPaWEDABzSIkSMgDKyNFeGc7V1iG+13CYTCb4+Pi0Op9kGxlBEKDT6ZCSkoI5c+YAuNSUaDQarF+/HhMnTrRquY5uZMg2tjYyUuXIRoaIiKxvZET9Qby6ujocP37c/PjkyZPIzc2Fv78/unTpglmzZuHFF19Ejx490K1bNzz33HPQ6XTmZoeIiIj+u4nayPzwww8YOXKk+fHs2bMBAElJSVi/fj3mzZuHs2fPYurUqaiursawYcOwc+dO8y+8EhER0X83yRxachQeWpIWHloiIiJrWHtoSXFXLREREdF/DzYyREREJFtsZIiIiEi22MgQERGRbLGRISIiItliI0NERESyxUaGiIiIZIuNDBEREckWGxkiIiKSLTYyREREJFtsZIiIiEi2RL1pZHu4fCupc3VnRa6EAKCuThn3WjrnwXstERE50uV/t693S0jF3zSyqKgIoaGhYpdBREREdigsLETnzp1bna74RqapqQnFxcXw9vaGSuWYv6JramoQGhqKwsLCa96hU8qUkAFgDilRQgZAGTmUkAFgDilpjwyCIKC2thY6nQ4dOrR+JoziDy116NDhmp1cW/Lx8ZHtm/IyJWQAmENKlJABUEYOJWQAmENKHJ3B19f3uvPwZF8iIiKSLTYyREREJFtsZNqAm5sbDAYD3NzcxC7FbkrIADCHlCghA6CMHErIADCHlEgpg+JP9iUiIiLl4h4ZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkblBK1euRNeuXeHu7o5Bgwbh+++/F7skm3zxxReIjY2FTqeDSqXCtm3bxC7JLpmZmbjtttvg7e0NjUaD++67D7///rvYZdlk9erV6NOnj/kHpgYPHowdO3aIXdYNeemll6BSqTBr1iyxS7FJWloaVCqVxRAZGSl2WXY5ffo0HnroIQQEBMDDwwO33HILfvjhB7HLsknXrl2bbQ+VSoXp06eLXZrVGhsb8dxzz6Fbt27w8PBAeHg4XnjhheveR0iKamtrMWvWLISFhcHDwwNDhgzBwYMHRauHjcwN2LRpE2bPng2DwYAff/wRffv2xejRo1FWViZ2aVY7e/Ys+vbti5UrV4pdyg3Zv38/pk+fjgMHDmD37t3466+/EB0djbNn5XOz0M6dO+Oll17CoUOH8MMPP+Cuu+7Cvffei99++03s0uxy8OBBrFmzBn369BG7FLv06tULJSUl5uGrr74SuySbVVVVYejQoXBxccGOHTtw5MgRLFmyBB07dhS7NJscPHjQYlvs3r0bADBhwgSRK7PeokWLsHr1arz22mvIy8vDokWLsHjxYqxYsULs0mz2yCOPYPfu3diwYQN++eUXREdHY9SoUTh9+rQ4BQlkt9tvv12YPn26+XFjY6Og0+mEzMxMEauyHwBh69atYpfRJsrKygQAwv79+8Uu5YZ07NhRePPNN8Uuw2a1tbVCjx49hN27dwvDhw8XZs6cKXZJNjEYDELfvn3FLuOGzZ8/Xxg2bJjYZbS5mTNnCuHh4UJTU5PYpVht7NixwpQpUyzG3X///cKkSZNEqsg+586dE5ycnISPP/7YYvytt94q/Pvf/xalJu6RsdPFixdx6NAhjBo1yjyuQ4cOGDVqFL799lsRKyMAMJlMAAB/f3+RK7FPY2MjNm7ciLNnz2Lw4MFil2Oz6dOnY+zYsRafD7k5duwYdDodunfvjkmTJqGgoEDskmz20UcfYeDAgZgwYQI0Gg369++PtWvXil3WDbl48SLeffddTJkyxWE3AnaEIUOGYO/evfjjjz8AAD///DO++uorxMTEiFyZbRoaGtDY2Ah3d3eL8R4eHqLttVT8TSMdpby8HI2NjQgKCrIYHxQUhKNHj4pUFQGX7ng+a9YsDB06FL179xa7HJv88ssvGDx4MM6fPw+1Wo2tW7fi5ptvFrssm2zcuBE//vijqMfMb9SgQYOwfv166PV6lJSUID09HXfeeSd+/fVXeHt7i12e1f7880+sXr0as2fPxjPPPIODBw/iySefhKurK5KSksQuzy7btm1DdXU1kpOTxS7FJk8//TRqamoQGRkJJycnNDY2YsGCBZg0aZLYpdnE29sbgwcPxgsvvICePXsiKCgI//u//4tvv/0WERERotTERoYUZ/r06fj1119leU6DXq9Hbm4uTCYTNm/ejKSkJOzfv182zUxhYSFmzpyJ3bt3N/uLTU6u/Cu5T58+GDRoEMLCwvDee+/h4YcfFrEy2zQ1NWHgwIFYuHAhAKB///749ddf8frrr8u2kXnrrbcQExMDnU4ndik2ee+995CdnY2cnBz06tULubm5mDVrFnQ6ney2xYYNGzBlyhSEhITAyckJt956Kx588EEcOnRIlHrYyNgpMDAQTk5OMBqNFuONRiO0Wq1IVdGMGTPw8ccf44svvkDnzp3FLsdmrq6u5r9qBgwYgIMHD2LZsmVYs2aNyJVZ59ChQygrK8Ott95qHtfY2IgvvvgCr732Gi5cuAAnJycRK7SPn58fbrrpJhw/flzsUmwSHBzcrAnu2bMnPvjgA5EqujGnTp3Cnj17sGXLFrFLsdncuXPx9NNPY+LEiQCAW265BadOnUJmZqbsGpnw8HDs378fZ8+eRU1NDYKDgxEfH4/u3buLUg/PkbGTq6srBgwYgL1795rHNTU1Ye/evbI8p0HuBEHAjBkzsHXrVnz22Wfo1q2b2CW1iaamJly4cEHsMqwWFRWFX375Bbm5ueZh4MCBmDRpEnJzc2XZxABAXV0dTpw4geDgYLFLscnQoUOb/QzBH3/8gbCwMJEqujHr1q2DRqPB2LFjxS7FZufOnUOHDpb/5Do5OaGpqUmkim6cl5cXgoODUVVVhV27duHee+8VpQ7ukbkBs2fPRlJSEgYOHIjbb78dWVlZOHv2LCZPnix2aVarq6uz+Cvz5MmTyM3Nhb+/P7p06SJiZbaZPn06cnJy8OGHH8Lb2xulpaUAAF9fX3h4eIhcnXVSU1MRExODLl26oLa2Fjk5Ofj888+xa9cusUuzmre3d7Pzkry8vBAQECCr85VSUlIQGxuLsLAwFBcXw2AwwMnJCQ8++KDYpdnkqaeewpAhQ7Bw4UI88MAD+P777/HGG2/gjTfeELs0mzU1NWHdunVISkqCs7P8/umKjY3FggUL0KVLF/Tq1Qs//fQTli5diilTpohdms127doFQRCg1+tx/PhxzJ07F5GRkeL92yfKtVIKsmLFCqFLly6Cq6urcPvttwsHDhwQuySb7Nu3TwDQbEhKShK7NJu0lAGAsG7dOrFLs9qUKVOEsLAwwdXVVejUqZMQFRUlfPrpp2KXdcPkePl1fHy8EBwcLLi6ugohISFCfHy8cPz4cbHLsst//vMfoXfv3oKbm5sQGRkpvPHGG2KXZJddu3YJAITff/9d7FLsUlNTI8ycOVPo0qWL4O7uLnTv3l3497//LVy4cEHs0my2adMmoXv37oKrq6ug1WqF6dOnC9XV1aLVoxIEGf6sIBERERF4jgwRERHJGBsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhohkZ/369fDz87vh5ahUKmzbtu2Gl0NE4mEjQ0SiSE5Oxn333Sd2GUQkc2xkiIiISLbYyBCR5CxduhS33HILvLy8EBoaiieeeAJ1dXXN5tu2bRt69OgBd3d3jB49GoWFhRbTP/zwQ9x6661wd3dH9+7dkZ6ejoaGhvaKQUTtgI0MEUlOhw4dsHz5cvz22294++238dlnn2HevHkW85w7dw4LFizAO++8g6+//hrV1dWYOHGiefqXX36JxMREzJw5E0eOHMGaNWuwfv16LFiwoL3jEJED8aaRRCSK5ORkVFdXW3Wy7ebNm/H444+jvLwcwKWTfSdPnowDBw5g0KBBAICjR4+iZ8+e+O6773D77bdj1KhRiIqKQmpqqnk57777LubNm4fi4mIAl0723bp1K8/VIZIxZ7ELICK62p49e5CZmYmjR4+ipqYGDQ0NOH/+PM6dOwdPT08AgLOzM2677TbzcyIjI+Hn54e8vDzcfvvt+Pnnn/H1119b7IFpbGxsthwikjc2MkQkKfn5+Rg3bhymTZuGBQsWwN/fH1999RUefvhhXLx40eoGpK6uDunp6bj//vubTXN3d2/rsolIJGxkiEhSDh06hKamJixZsgQdOlw6je+9995rNl9DQwN++OEH3H777QCA33//HdXV1ejZsycA4NZbb8Xvv/+OiIiI9iueiNodGxkiEo3JZEJubq7FuMDAQPz1119YsWIFYmNj8fXXX+P1119v9lwXFxf861//wvLly+Hs7IwZM2bgjjvuMDc2zz//PMaNG4cuXbpg/Pjx6NChA37++Wf8+uuvePHFF9sjHhG1A161RESi+fzzz9G/f3+LYcOGDVi6dCkWLVqE3r17Izs7G5mZmc2e6+npifnz5yMhIQFDhw6FWq3Gpk2bzNNHjx6Njz/+GJ9++iluu+023HHHHXj11VcRFhbWnhGJyMF41RIRERHJFvfIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGSLjQwRERHJFhsZIiIiki02MkRERCRbbGSIiIhIttjIEBERkWyxkSEiIiLZYiNDREREssVGhoiIiGTr/wGUVkjWIHv0dgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create an array to store the histogram data for each partition\n",
        "hist_data = [np.histogram(partition, bins=range(11))[0] for partition in train_label_part]\n",
        "\n",
        "# Plot heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(hist_data, cmap='Pastel2',aspect='auto', interpolation='none')\n",
        "\n",
        "# Add annotations\n",
        "for i in range(len(train_label_part)):\n",
        "    for j in range(10):\n",
        "        text = ax.text(j, i, hist_data[i][j], ha='center', va='center', color='black', fontsize=8)\n",
        "\n",
        "# Set labels and ticks\n",
        "ax.set_title('Distribution of Labels in Train Partitions (Heatmap)')\n",
        "ax.set_xlabel('Label')\n",
        "ax.set_ylabel('Partition')\n",
        "ax.set_xticks(range(10))\n",
        "ax.set_yticks(range(len(train_label_part)))\n",
        "ax.set_xticklabels(range(10))\n",
        "ax.set_yticklabels(range(1, len(train_label_part) + 1))\n",
        "\n",
        "# Add colorbar\n",
        "plt.colorbar(im, ax=ax).remove()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mw-D3oXLV3fO"
      },
      "outputs": [],
      "source": [
        "def simpleRNNModel():\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Reshape((32, 32 * 3), input_shape=(32, 32, 3)))\n",
        "    model.add(keras.layers.LSTM(128))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(10,activation=\"softmax\"))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),loss= \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6B7k1K8V3fO"
      },
      "source": [
        "# Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "trRKge-3V3fO"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, train_x,train_y,val_x,val_y, model) -> None:\n",
        "        super().__init__()\n",
        "        self.train_feature_set = train_x\n",
        "        self.train_label_set = train_y\n",
        "        self.val_feature_set = val_x\n",
        "        self.val_label_set = val_y\n",
        "        self.model = model\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        print(f\"round :{config['current_round']}\")\n",
        "        print(f\"Train :{len(self.train_feature_set)}\")\n",
        "        print(f\"Test :{len(self.val_feature_set)}\")\n",
        "\n",
        "        # Train the returned model\n",
        "        TrainHistory = self.model.fit(x=self.train_feature_set,\n",
        "                                 y=self.train_label_set,\n",
        "                                 verbose=2,\n",
        "                                 batch_size=config['batch_size'],\n",
        "                                 epochs=config['local_epochs'],\n",
        "                                 validation_data =(self.val_feature_set,self.val_label_set)\n",
        "                                 )\n",
        "        result ={\n",
        "                \"loss\": TrainHistory.history['loss'][-1],\n",
        "                \"accuracy\": round(TrainHistory.history['accuracy'][-1]*100,2),\n",
        "                \"val_loss\" : TrainHistory.history['val_loss'][-1],\n",
        "                \"val_accuracy\"  : round(TrainHistory.history[\"val_accuracy\"][-1]*100,2),\n",
        "        }\n",
        "\n",
        "        print(\"Client Train and validate Done!!!\")\n",
        "        # Send local model to server to do aggreation\n",
        "        return self.model.get_weights(), len(self.train_feature_set),result\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        # Test Global model\n",
        "        loss, accuracy = self.model.evaluate(self.val_feature_set,\n",
        "                                        self.val_label_set,\n",
        "                                        batch_size=config['batch_size'],\n",
        "                                        verbose=2)\n",
        "        print(\"Global Validate Done!!!\")\n",
        "        return loss, len(self.val_feature_set), {\"accuracy\": round(float(accuracy*100),2)}\n",
        "\n",
        "\n",
        "def generate_client_fn(train_x,train_y,val_x,val_y, model):\n",
        "    \"\"\"Return a function to construct a FlowerClient.\"\"\"\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        print\n",
        "        return FlowerClient(train_x[int(cid)],train_y[int(cid)],val_x,val_y,model)\n",
        "    return client_fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmnSX-EaV3fP"
      },
      "source": [
        "# Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DgMcC2bbV3fP"
      },
      "outputs": [],
      "source": [
        "# config for Client using Server side\n",
        "def fit_config(server_round: int):\n",
        "    \"\"\"Return training configuration dict for each round.\"\"\"\n",
        "    config = {\n",
        "        \"batch_size\": 32,\n",
        "        \"current_round\": server_round,\n",
        "        \"local_epochs\": 10,\n",
        "    }\n",
        "    return config\n",
        "\n",
        "def evalute_config(server_round: int):\n",
        "    \"\"\"Return test configuration dict for each round.\"\"\"\n",
        "    config = {\n",
        "        \"batch_size\": 1,\n",
        "        \"current_round\": server_round,\n",
        "    }\n",
        "    return config\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
        "\n",
        "def fit_average(metric):\n",
        "    examples = [num_examples for num_examples, _ in metric]\n",
        "    trainacc = [num_examples * m[\"accuracy\"] for num_examples, m in metric]\n",
        "    valacc = [num_examples * m[\"val_accuracy\"] for num_examples, m in metric]\n",
        "\n",
        "    return { \"example\": sum(examples),\n",
        "            \"accuracy\": sum(trainacc) / sum(examples),\n",
        "            \"valacc\": sum(valacc) / sum(examples)\n",
        "            }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rlic7AGPV3fP"
      },
      "outputs": [],
      "source": [
        "Fedavgconfig = fl.server.strategy.FedAvg(on_fit_config_fn=fit_config,\n",
        "                                         on_evaluate_config_fn=evalute_config,\n",
        "                                         evaluate_metrics_aggregation_fn=weighted_average,\n",
        "                                         fit_metrics_aggregation_fn=fit_average,\n",
        "                                         min_evaluate_clients= NumOfPartition,\n",
        "                                         min_fit_clients=NumOfPartition,\n",
        "                                         min_available_clients=NumOfPartition,\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BNL6T6yV3fP"
      },
      "source": [
        "# Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KTqanKskV3fP"
      },
      "outputs": [],
      "source": [
        "globalmodel = simpleRNNModel()\n",
        "clientfn = generate_client_fn(train_image_part,train_label_part,Test_images_n,Test_label,globalmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PFO5ktD_V3fP",
        "outputId": "ad828222-49f7-41c8-b765-d60ccfc137fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2024-02-12 03:44:47,154 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "2024-02-12 03:44:54,016\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2024-02-12 03:44:55,968 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 7870655694.0, 'object_store_memory': 3935327846.0, 'CPU': 2.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'memory': 7870655694.0, 'object_store_memory': 3935327846.0, 'CPU': 2.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0}\n",
            "INFO flwr 2024-02-12 03:44:55,972 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO flwr 2024-02-12 03:44:55,974 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1}\n",
            "INFO flwr 2024-02-12 03:44:55,997 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "INFO flwr 2024-02-12 03:44:56,005 | server.py:89 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2024-02-12 03:44:56,011 | server.py:276 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=8846)\u001b[0m 2024-02-12 03:44:58.350139: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=8846)\u001b[0m 2024-02-12 03:44:58.350208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=8846)\u001b[0m 2024-02-12 03:44:58.351548: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=8846)\u001b[0m 2024-02-12 03:44:59.651889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "INFO flwr 2024-02-12 03:45:02,866 | server.py:280 | Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "INFO flwr 2024-02-12 03:45:02,874 | server.py:91 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2024-02-12 03:45:02,878 | server.py:104 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2024-02-12 03:45:02,880 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 10 clients (out of 10)\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m /usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m I0000 00:00:1707709506.930093    8977 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0392 - accuracy: 0.9936 - val_loss: 10.2030 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.3053e-05 - accuracy: 1.0000 - val_loss: 10.8755 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9312e-05 - accuracy: 1.0000 - val_loss: 11.3179 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1988e-05 - accuracy: 1.0000 - val_loss: 11.6363 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3052e-05 - accuracy: 1.0000 - val_loss: 11.8882 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7648e-05 - accuracy: 1.0000 - val_loss: 12.1006 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4042e-05 - accuracy: 1.0000 - val_loss: 12.2857 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1479e-05 - accuracy: 1.0000 - val_loss: 12.4518 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.5760e-06 - accuracy: 1.0000 - val_loss: 12.6033 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.1151e-06 - accuracy: 1.0000 - val_loss: 12.7425 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0250 - accuracy: 0.9980 - val_loss: 10.6414 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.5619e-05 - accuracy: 1.0000 - val_loss: 11.2428 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1660e-05 - accuracy: 1.0000 - val_loss: 11.6585 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1017e-05 - accuracy: 1.0000 - val_loss: 11.9789 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5155e-05 - accuracy: 1.0000 - val_loss: 12.2435 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1505e-05 - accuracy: 1.0000 - val_loss: 12.4691 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.0813e-06 - accuracy: 1.0000 - val_loss: 12.6644 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.3929e-06 - accuracy: 1.0000 - val_loss: 12.8360 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1572e-06 - accuracy: 1.0000 - val_loss: 12.9899 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.2140e-06 - accuracy: 1.0000 - val_loss: 13.1302 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0298 - accuracy: 0.9962 - val_loss: 10.4145 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.1987e-05 - accuracy: 1.0000 - val_loss: 11.0779 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9064e-05 - accuracy: 1.0000 - val_loss: 11.4926 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5748e-05 - accuracy: 1.0000 - val_loss: 11.8075 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8529e-05 - accuracy: 1.0000 - val_loss: 12.0675 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4069e-05 - accuracy: 1.0000 - val_loss: 12.2886 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1124e-05 - accuracy: 1.0000 - val_loss: 12.4801 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.0592e-06 - accuracy: 1.0000 - val_loss: 12.6500 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.5382e-06 - accuracy: 1.0000 - val_loss: 12.8032 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.3809e-06 - accuracy: 1.0000 - val_loss: 12.9435 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0459 - accuracy: 0.9934 - val_loss: 9.9501 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0866e-04 - accuracy: 1.0000 - val_loss: 10.5638 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1212e-05 - accuracy: 1.0000 - val_loss: 10.9749 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.0780e-05 - accuracy: 1.0000 - val_loss: 11.2872 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9732e-05 - accuracy: 1.0000 - val_loss: 11.5403 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2913e-05 - accuracy: 1.0000 - val_loss: 11.7550 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8314e-05 - accuracy: 1.0000 - val_loss: 11.9430 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5022e-05 - accuracy: 1.0000 - val_loss: 12.1113 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2559e-05 - accuracy: 1.0000 - val_loss: 12.2647 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0657e-05 - accuracy: 1.0000 - val_loss: 12.4064 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0283 - accuracy: 0.9940 - val_loss: 10.1803 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.0263e-05 - accuracy: 1.0000 - val_loss: 10.8355 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8045e-05 - accuracy: 1.0000 - val_loss: 11.2908 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0511e-05 - accuracy: 1.0000 - val_loss: 11.6432 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1341e-05 - accuracy: 1.0000 - val_loss: 11.9290 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5918e-05 - accuracy: 1.0000 - val_loss: 12.1673 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2415e-05 - accuracy: 1.0000 - val_loss: 12.3721 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0000e-05 - accuracy: 1.0000 - val_loss: 12.5518 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.2523e-06 - accuracy: 1.0000 - val_loss: 12.7127 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.9428e-06 - accuracy: 1.0000 - val_loss: 12.8578 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0424 - accuracy: 0.9936 - val_loss: 10.1008 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.8051e-05 - accuracy: 1.0000 - val_loss: 10.7752 - val_accuracy: 0.1000 - 2s/epoch - 16ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.2059e-05 - accuracy: 1.0000 - val_loss: 11.2313 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3287e-05 - accuracy: 1.0000 - val_loss: 11.5731 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3554e-05 - accuracy: 1.0000 - val_loss: 11.8466 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7739e-05 - accuracy: 1.0000 - val_loss: 12.0760 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3924e-05 - accuracy: 1.0000 - val_loss: 12.2752 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1253e-05 - accuracy: 1.0000 - val_loss: 12.4520 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.3065e-06 - accuracy: 1.0000 - val_loss: 12.6110 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.8363e-06 - accuracy: 1.0000 - val_loss: 12.7558 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0389 - accuracy: 0.9936 - val_loss: 10.0251 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0950e-04 - accuracy: 1.0000 - val_loss: 10.6932 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.9207e-05 - accuracy: 1.0000 - val_loss: 11.1219 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 3.8599e-05 - accuracy: 1.0000 - val_loss: 11.4471 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7563e-05 - accuracy: 1.0000 - val_loss: 11.7123 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0783e-05 - accuracy: 1.0000 - val_loss: 11.9376 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6246e-05 - accuracy: 1.0000 - val_loss: 12.1343 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3070e-05 - accuracy: 1.0000 - val_loss: 12.3096 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0778e-05 - accuracy: 1.0000 - val_loss: 12.4679 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.0587e-06 - accuracy: 1.0000 - val_loss: 12.6126 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0390 - accuracy: 0.9936 - val_loss: 10.0245 - val_accuracy: 0.1000 - 5s/epoch - 32ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.5336e-05 - accuracy: 1.0000 - val_loss: 10.8038 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.6592e-05 - accuracy: 1.0000 - val_loss: 11.2906 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8971e-05 - accuracy: 1.0000 - val_loss: 11.6409 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0355e-05 - accuracy: 1.0000 - val_loss: 11.9137 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5357e-05 - accuracy: 1.0000 - val_loss: 12.1392 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2122e-05 - accuracy: 1.0000 - val_loss: 12.3340 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.8686e-06 - accuracy: 1.0000 - val_loss: 12.5060 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 1s - loss: 8.2150e-06 - accuracy: 1.0000 - val_loss: 12.6609 - val_accuracy: 0.1000 - 1s/epoch - 9ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.9579e-06 - accuracy: 1.0000 - val_loss: 12.8023 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0464 - accuracy: 0.9930 - val_loss: 9.7959 - val_accuracy: 0.1000 - 5s/epoch - 30ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3083e-04 - accuracy: 1.0000 - val_loss: 10.4850 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.7899e-05 - accuracy: 1.0000 - val_loss: 10.9554 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2694e-05 - accuracy: 1.0000 - val_loss: 11.3118 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9770e-05 - accuracy: 1.0000 - val_loss: 11.6007 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2146e-05 - accuracy: 1.0000 - val_loss: 11.8428 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7236e-05 - accuracy: 1.0000 - val_loss: 12.0512 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3857e-05 - accuracy: 1.0000 - val_loss: 12.2344 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1413e-05 - accuracy: 1.0000 - val_loss: 12.3983 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.5792e-06 - accuracy: 1.0000 - val_loss: 12.5468 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :1\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0488 - accuracy: 0.9932 - val_loss: 10.0592 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.9475e-05 - accuracy: 1.0000 - val_loss: 10.7571 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1309e-05 - accuracy: 1.0000 - val_loss: 11.2006 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2883e-05 - accuracy: 1.0000 - val_loss: 11.5442 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3197e-05 - accuracy: 1.0000 - val_loss: 11.8182 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7555e-05 - accuracy: 1.0000 - val_loss: 12.0406 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3875e-05 - accuracy: 1.0000 - val_loss: 12.2325 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1288e-05 - accuracy: 1.0000 - val_loss: 12.4028 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.3856e-06 - accuracy: 1.0000 - val_loss: 12.5567 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 03:48:41,223 | server.py:236 | fit_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 03:48:41,260 | server.py:173 | evaluate_round 1: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.9340e-06 - accuracy: 1.0000 - val_loss: 12.6977 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.3924 - accuracy: 0.1135 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3924 - accuracy: 0.1135 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 03:52:58,929 | server.py:187 | evaluate_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 03:52:58,934 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.3924 - accuracy: 0.1135 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0226 - accuracy: 0.9950 - val_loss: 9.5492 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9979e-04 - accuracy: 1.0000 - val_loss: 10.2554 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0223e-04 - accuracy: 1.0000 - val_loss: 10.7151 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.4699e-05 - accuracy: 1.0000 - val_loss: 11.0502 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5682e-05 - accuracy: 1.0000 - val_loss: 11.3162 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4383e-05 - accuracy: 1.0000 - val_loss: 11.5401 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6976e-05 - accuracy: 1.0000 - val_loss: 11.7351 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1805e-05 - accuracy: 1.0000 - val_loss: 11.9083 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8022e-05 - accuracy: 1.0000 - val_loss: 12.0651 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5163e-05 - accuracy: 1.0000 - val_loss: 12.2087 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0340 - accuracy: 0.9936 - val_loss: 8.9881 - val_accuracy: 0.1000 - 5s/epoch - 32ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0674e-04 - accuracy: 1.0000 - val_loss: 9.7674 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5005e-04 - accuracy: 1.0000 - val_loss: 10.2602 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.2455e-05 - accuracy: 1.0000 - val_loss: 10.6244 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.3869e-05 - accuracy: 1.0000 - val_loss: 10.9152 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7282e-05 - accuracy: 1.0000 - val_loss: 11.1577 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6667e-05 - accuracy: 1.0000 - val_loss: 11.3662 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9401e-05 - accuracy: 1.0000 - val_loss: 11.5492 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4173e-05 - accuracy: 1.0000 - val_loss: 11.7126 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0265e-05 - accuracy: 1.0000 - val_loss: 11.8615 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0379 - accuracy: 0.9936 - val_loss: 9.3637 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3356e-04 - accuracy: 1.0000 - val_loss: 10.1077 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1790e-04 - accuracy: 1.0000 - val_loss: 10.5833 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.4144e-05 - accuracy: 1.0000 - val_loss: 10.9308 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.2092e-05 - accuracy: 1.0000 - val_loss: 11.2061 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9049e-05 - accuracy: 1.0000 - val_loss: 11.4373 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0556e-05 - accuracy: 1.0000 - val_loss: 11.6375 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4639e-05 - accuracy: 1.0000 - val_loss: 11.8158 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0314e-05 - accuracy: 1.0000 - val_loss: 11.9775 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7033e-05 - accuracy: 1.0000 - val_loss: 12.1259 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0265 - accuracy: 0.9936 - val_loss: 9.4652 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0020e-04 - accuracy: 1.0000 - val_loss: 10.2019 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.9711e-05 - accuracy: 1.0000 - val_loss: 10.6823 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1670e-05 - accuracy: 1.0000 - val_loss: 11.0425 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2572e-05 - accuracy: 1.0000 - val_loss: 11.3331 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1434e-05 - accuracy: 1.0000 - val_loss: 11.5760 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4277e-05 - accuracy: 1.0000 - val_loss: 11.7872 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9350e-05 - accuracy: 1.0000 - val_loss: 11.9753 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5786e-05 - accuracy: 1.0000 - val_loss: 12.1457 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3112e-05 - accuracy: 1.0000 - val_loss: 12.3024 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0224 - accuracy: 0.9946 - val_loss: 9.7240 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6621e-04 - accuracy: 1.0000 - val_loss: 10.4355 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.3608e-05 - accuracy: 1.0000 - val_loss: 10.9127 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1959e-05 - accuracy: 1.0000 - val_loss: 11.2644 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6208e-05 - accuracy: 1.0000 - val_loss: 11.5454 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6984e-05 - accuracy: 1.0000 - val_loss: 11.7792 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1039e-05 - accuracy: 1.0000 - val_loss: 11.9809 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6943e-05 - accuracy: 1.0000 - val_loss: 12.1583 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3998e-05 - accuracy: 1.0000 - val_loss: 12.3159 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1788e-05 - accuracy: 1.0000 - val_loss: 12.4590 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0211 - accuracy: 0.9964 - val_loss: 9.4536 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0810e-04 - accuracy: 1.0000 - val_loss: 10.1622 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0655e-04 - accuracy: 1.0000 - val_loss: 10.6317 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.6392e-05 - accuracy: 1.0000 - val_loss: 10.9921 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5939e-05 - accuracy: 1.0000 - val_loss: 11.2849 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3825e-05 - accuracy: 1.0000 - val_loss: 11.5323 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6088e-05 - accuracy: 1.0000 - val_loss: 11.7443 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0829e-05 - accuracy: 1.0000 - val_loss: 11.9298 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7054e-05 - accuracy: 1.0000 - val_loss: 12.0968 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4237e-05 - accuracy: 1.0000 - val_loss: 12.2481 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0320 - accuracy: 0.9936 - val_loss: 9.4949 - val_accuracy: 0.1000 - 5s/epoch - 30ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9711e-04 - accuracy: 1.0000 - val_loss: 10.2054 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0074e-04 - accuracy: 1.0000 - val_loss: 10.6773 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.3410e-05 - accuracy: 1.0000 - val_loss: 11.0235 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.4452e-05 - accuracy: 1.0000 - val_loss: 11.2998 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3279e-05 - accuracy: 1.0000 - val_loss: 11.5310 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6008e-05 - accuracy: 1.0000 - val_loss: 11.7310 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0943e-05 - accuracy: 1.0000 - val_loss: 11.9087 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7261e-05 - accuracy: 1.0000 - val_loss: 12.0684 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4477e-05 - accuracy: 1.0000 - val_loss: 12.2152 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0223 - accuracy: 0.9936 - val_loss: 9.4491 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1417e-04 - accuracy: 1.0000 - val_loss: 10.1034 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1393e-04 - accuracy: 1.0000 - val_loss: 10.5393 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.2870e-05 - accuracy: 1.0000 - val_loss: 10.8691 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1097e-05 - accuracy: 1.0000 - val_loss: 11.1376 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7856e-05 - accuracy: 1.0000 - val_loss: 11.3686 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9495e-05 - accuracy: 1.0000 - val_loss: 11.5705 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3732e-05 - accuracy: 1.0000 - val_loss: 11.7508 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9528e-05 - accuracy: 1.0000 - val_loss: 11.9151 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6357e-05 - accuracy: 1.0000 - val_loss: 12.0659 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0264 - accuracy: 0.9938 - val_loss: 9.1877 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5422e-04 - accuracy: 1.0000 - val_loss: 9.9041 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2919e-04 - accuracy: 1.0000 - val_loss: 10.3849 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.9809e-05 - accuracy: 1.0000 - val_loss: 10.7505 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.4723e-05 - accuracy: 1.0000 - val_loss: 11.0463 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.0144e-05 - accuracy: 1.0000 - val_loss: 11.2929 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0922e-05 - accuracy: 1.0000 - val_loss: 11.5053 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4686e-05 - accuracy: 1.0000 - val_loss: 11.6914 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0222e-05 - accuracy: 1.0000 - val_loss: 11.8583 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6876e-05 - accuracy: 1.0000 - val_loss: 12.0114 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :2\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0201 - accuracy: 0.9954 - val_loss: 9.3499 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1588e-04 - accuracy: 1.0000 - val_loss: 10.1386 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0326e-04 - accuracy: 1.0000 - val_loss: 10.6829 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1804e-05 - accuracy: 1.0000 - val_loss: 11.0663 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2293e-05 - accuracy: 1.0000 - val_loss: 11.3607 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1280e-05 - accuracy: 1.0000 - val_loss: 11.6001 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4329e-05 - accuracy: 1.0000 - val_loss: 11.8045 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9578e-05 - accuracy: 1.0000 - val_loss: 11.9833 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6149e-05 - accuracy: 1.0000 - val_loss: 12.1435 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 03:56:35,029 | server.py:236 | fit_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 03:56:35,050 | server.py:173 | evaluate_round 2: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3565e-05 - accuracy: 1.0000 - val_loss: 12.2897 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4202 - accuracy: 0.1289 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4202 - accuracy: 0.1289 - 25s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4202 - accuracy: 0.1289 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4202 - accuracy: 0.1289 - 25s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4202 - accuracy: 0.1289 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4202 - accuracy: 0.1289 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4202 - accuracy: 0.1289 - 25s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4202 - accuracy: 0.1289 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4202 - accuracy: 0.1289 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:00:56,628 | server.py:187 | evaluate_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:00:56,632 | server.py:222 | fit_round 3: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4202 - accuracy: 0.1289 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0178 - accuracy: 0.9964 - val_loss: 9.2738 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8676e-04 - accuracy: 1.0000 - val_loss: 9.9726 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4761e-04 - accuracy: 1.0000 - val_loss: 10.4366 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.2533e-05 - accuracy: 1.0000 - val_loss: 10.7840 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.4519e-05 - accuracy: 1.0000 - val_loss: 11.0630 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8020e-05 - accuracy: 1.0000 - val_loss: 11.2969 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7403e-05 - accuracy: 1.0000 - val_loss: 11.4997 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0064e-05 - accuracy: 1.0000 - val_loss: 11.6789 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4736e-05 - accuracy: 1.0000 - val_loss: 11.8416 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0717e-05 - accuracy: 1.0000 - val_loss: 11.9907 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0204 - accuracy: 0.9936 - val_loss: 9.3535 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5866e-04 - accuracy: 1.0000 - val_loss: 10.0485 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3692e-04 - accuracy: 1.0000 - val_loss: 10.4879 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.8106e-05 - accuracy: 1.0000 - val_loss: 10.8177 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.2714e-05 - accuracy: 1.0000 - val_loss: 11.0798 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7446e-05 - accuracy: 1.0000 - val_loss: 11.3007 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7362e-05 - accuracy: 1.0000 - val_loss: 11.4935 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0281e-05 - accuracy: 1.0000 - val_loss: 11.6652 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5075e-05 - accuracy: 1.0000 - val_loss: 11.8211 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1109e-05 - accuracy: 1.0000 - val_loss: 11.9647 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0215 - accuracy: 0.9938 - val_loss: 9.4945 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0338e-04 - accuracy: 1.0000 - val_loss: 10.1625 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0713e-04 - accuracy: 1.0000 - val_loss: 10.6180 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.8280e-05 - accuracy: 1.0000 - val_loss: 10.9568 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8276e-05 - accuracy: 1.0000 - val_loss: 11.2264 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6366e-05 - accuracy: 1.0000 - val_loss: 11.4522 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8562e-05 - accuracy: 1.0000 - val_loss: 11.6472 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3142e-05 - accuracy: 1.0000 - val_loss: 11.8194 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9188e-05 - accuracy: 1.0000 - val_loss: 11.9751 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6182e-05 - accuracy: 1.0000 - val_loss: 12.1167 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0308 - accuracy: 0.9936 - val_loss: 9.5214 - val_accuracy: 0.1000 - 4s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0130e-04 - accuracy: 1.0000 - val_loss: 10.2133 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0792e-04 - accuracy: 1.0000 - val_loss: 10.6547 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 6.9924e-05 - accuracy: 1.0000 - val_loss: 10.9824 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9894e-05 - accuracy: 1.0000 - val_loss: 11.2469 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7768e-05 - accuracy: 1.0000 - val_loss: 11.4668 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9771e-05 - accuracy: 1.0000 - val_loss: 11.6593 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4145e-05 - accuracy: 1.0000 - val_loss: 11.8296 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0031e-05 - accuracy: 1.0000 - val_loss: 11.9828 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6896e-05 - accuracy: 1.0000 - val_loss: 12.1227 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0316 - accuracy: 0.9936 - val_loss: 8.8094 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9531e-04 - accuracy: 1.0000 - val_loss: 9.5183 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0205e-04 - accuracy: 1.0000 - val_loss: 10.0055 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2546e-04 - accuracy: 1.0000 - val_loss: 10.3692 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.6816e-05 - accuracy: 1.0000 - val_loss: 10.6606 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.4289e-05 - accuracy: 1.0000 - val_loss: 10.9023 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9874e-05 - accuracy: 1.0000 - val_loss: 11.1106 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9999e-05 - accuracy: 1.0000 - val_loss: 11.2920 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2885e-05 - accuracy: 1.0000 - val_loss: 11.4544 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7553e-05 - accuracy: 1.0000 - val_loss: 11.6025 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0222 - accuracy: 0.9938 - val_loss: 9.5375 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3944e-04 - accuracy: 1.0000 - val_loss: 10.1219 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3400e-04 - accuracy: 1.0000 - val_loss: 10.5391 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.6925e-05 - accuracy: 1.0000 - val_loss: 10.8755 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1173e-05 - accuracy: 1.0000 - val_loss: 11.1531 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5831e-05 - accuracy: 1.0000 - val_loss: 11.3783 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6018e-05 - accuracy: 1.0000 - val_loss: 11.5687 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9232e-05 - accuracy: 1.0000 - val_loss: 11.7351 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4277e-05 - accuracy: 1.0000 - val_loss: 11.8853 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0510e-05 - accuracy: 1.0000 - val_loss: 12.0227 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0206 - accuracy: 0.9944 - val_loss: 9.5994 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1945e-04 - accuracy: 1.0000 - val_loss: 10.1822 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2391e-04 - accuracy: 1.0000 - val_loss: 10.5881 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.1696e-05 - accuracy: 1.0000 - val_loss: 10.9026 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.8813e-05 - accuracy: 1.0000 - val_loss: 11.1586 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.4724e-05 - accuracy: 1.0000 - val_loss: 11.3775 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5301e-05 - accuracy: 1.0000 - val_loss: 11.5699 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8618e-05 - accuracy: 1.0000 - val_loss: 11.7427 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3683e-05 - accuracy: 1.0000 - val_loss: 11.9009 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9909e-05 - accuracy: 1.0000 - val_loss: 12.0466 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0189 - accuracy: 0.9940 - val_loss: 9.9541 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4847e-04 - accuracy: 1.0000 - val_loss: 10.6196 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.8679e-05 - accuracy: 1.0000 - val_loss: 11.0614 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 5.0401e-05 - accuracy: 1.0000 - val_loss: 11.3877 - val_accuracy: 0.1000 - 3s/epoch - 17ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6085e-05 - accuracy: 1.0000 - val_loss: 11.6365 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7576e-05 - accuracy: 1.0000 - val_loss: 11.8426 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1970e-05 - accuracy: 1.0000 - val_loss: 12.0190 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8016e-05 - accuracy: 1.0000 - val_loss: 12.1757 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5083e-05 - accuracy: 1.0000 - val_loss: 12.3174 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2830e-05 - accuracy: 1.0000 - val_loss: 12.4475 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0281 - accuracy: 0.9936 - val_loss: 9.4527 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2586e-04 - accuracy: 1.0000 - val_loss: 10.0565 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2653e-04 - accuracy: 1.0000 - val_loss: 10.4620 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.3739e-05 - accuracy: 1.0000 - val_loss: 10.7738 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.0523e-05 - accuracy: 1.0000 - val_loss: 11.0275 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.6237e-05 - accuracy: 1.0000 - val_loss: 11.2432 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6669e-05 - accuracy: 1.0000 - val_loss: 11.4325 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9887e-05 - accuracy: 1.0000 - val_loss: 11.6009 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4871e-05 - accuracy: 1.0000 - val_loss: 11.7541 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1024e-05 - accuracy: 1.0000 - val_loss: 11.8956 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :3\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0159 - accuracy: 0.9984 - val_loss: 9.3932 - val_accuracy: 0.1000 - 5s/epoch - 30ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2555e-04 - accuracy: 1.0000 - val_loss: 10.0910 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1316e-04 - accuracy: 1.0000 - val_loss: 10.6092 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.7233e-05 - accuracy: 1.0000 - val_loss: 10.9955 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5480e-05 - accuracy: 1.0000 - val_loss: 11.2927 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3304e-05 - accuracy: 1.0000 - val_loss: 11.5365 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5694e-05 - accuracy: 1.0000 - val_loss: 11.7439 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0543e-05 - accuracy: 1.0000 - val_loss: 11.9248 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6856e-05 - accuracy: 1.0000 - val_loss: 12.0873 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 04:04:39,245 | server.py:236 | fit_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:04:39,271 | server.py:173 | evaluate_round 3: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4102e-05 - accuracy: 1.0000 - val_loss: 12.2351 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3660 - accuracy: 0.1278 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3660 - accuracy: 0.1278 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.3660 - accuracy: 0.1278 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3660 - accuracy: 0.1278 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3660 - accuracy: 0.1278 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.3660 - accuracy: 0.1278 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3660 - accuracy: 0.1278 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3660 - accuracy: 0.1278 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.3660 - accuracy: 0.1278 - 25s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:09:02,103 | server.py:187 | evaluate_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:09:02,106 | server.py:222 | fit_round 4: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.3660 - accuracy: 0.1278 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0153 - accuracy: 0.9944 - val_loss: 9.7440 - val_accuracy: 0.1000 - 5s/epoch - 35ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5964e-04 - accuracy: 1.0000 - val_loss: 10.4335 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.7099e-05 - accuracy: 1.0000 - val_loss: 10.8406 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.8991e-05 - accuracy: 1.0000 - val_loss: 11.1302 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.3588e-05 - accuracy: 1.0000 - val_loss: 11.3690 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3806e-05 - accuracy: 1.0000 - val_loss: 11.5714 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7098e-05 - accuracy: 1.0000 - val_loss: 11.7514 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2269e-05 - accuracy: 1.0000 - val_loss: 11.9134 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8645e-05 - accuracy: 1.0000 - val_loss: 12.0603 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5849e-05 - accuracy: 1.0000 - val_loss: 12.1961 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0184 - accuracy: 0.9940 - val_loss: 9.4402 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1324e-04 - accuracy: 1.0000 - val_loss: 10.0657 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1632e-04 - accuracy: 1.0000 - val_loss: 10.4859 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.5889e-05 - accuracy: 1.0000 - val_loss: 10.8062 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.4468e-05 - accuracy: 1.0000 - val_loss: 11.0654 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1444e-05 - accuracy: 1.0000 - val_loss: 11.2860 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2779e-05 - accuracy: 1.0000 - val_loss: 11.4782 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6653e-05 - accuracy: 1.0000 - val_loss: 11.6509 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2123e-05 - accuracy: 1.0000 - val_loss: 11.8077 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8668e-05 - accuracy: 1.0000 - val_loss: 11.9518 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0162 - accuracy: 0.9952 - val_loss: 9.7173 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7649e-04 - accuracy: 1.0000 - val_loss: 10.3566 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.8147e-05 - accuracy: 1.0000 - val_loss: 10.7543 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 6.4805e-05 - accuracy: 1.0000 - val_loss: 11.0720 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5788e-05 - accuracy: 1.0000 - val_loss: 11.3594 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4481e-05 - accuracy: 1.0000 - val_loss: 11.5761 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7671e-05 - accuracy: 1.0000 - val_loss: 11.7474 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2944e-05 - accuracy: 1.0000 - val_loss: 11.8973 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9400e-05 - accuracy: 1.0000 - val_loss: 12.0341 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6638e-05 - accuracy: 1.0000 - val_loss: 12.1604 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0216 - accuracy: 0.9936 - val_loss: 9.6568 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0838e-04 - accuracy: 1.0000 - val_loss: 10.2227 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1756e-04 - accuracy: 1.0000 - val_loss: 10.6252 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.6839e-05 - accuracy: 1.0000 - val_loss: 10.9391 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.4818e-05 - accuracy: 1.0000 - val_loss: 11.1931 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1439e-05 - accuracy: 1.0000 - val_loss: 11.4078 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2611e-05 - accuracy: 1.0000 - val_loss: 11.5951 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6427e-05 - accuracy: 1.0000 - val_loss: 11.7626 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1877e-05 - accuracy: 1.0000 - val_loss: 11.9157 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8407e-05 - accuracy: 1.0000 - val_loss: 12.0580 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0218 - accuracy: 0.9940 - val_loss: 9.0171 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4319e-04 - accuracy: 1.0000 - val_loss: 9.7029 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7994e-04 - accuracy: 1.0000 - val_loss: 10.1481 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1425e-04 - accuracy: 1.0000 - val_loss: 10.4905 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.9654e-05 - accuracy: 1.0000 - val_loss: 10.7777 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.6998e-05 - accuracy: 1.0000 - val_loss: 11.0733 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2246e-05 - accuracy: 1.0000 - val_loss: 11.3028 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3542e-05 - accuracy: 1.0000 - val_loss: 11.4942 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7414e-05 - accuracy: 1.0000 - val_loss: 11.6658 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2847e-05 - accuracy: 1.0000 - val_loss: 11.8226 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0232 - accuracy: 0.9936 - val_loss: 9.1871 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1084e-04 - accuracy: 1.0000 - val_loss: 9.9048 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5800e-04 - accuracy: 1.0000 - val_loss: 10.3682 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.9213e-05 - accuracy: 1.0000 - val_loss: 10.7080 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.9434e-05 - accuracy: 1.0000 - val_loss: 10.9790 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1947e-05 - accuracy: 1.0000 - val_loss: 11.2054 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.0587e-05 - accuracy: 1.0000 - val_loss: 11.4024 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2703e-05 - accuracy: 1.0000 - val_loss: 11.5773 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6963e-05 - accuracy: 1.0000 - val_loss: 11.7363 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2625e-05 - accuracy: 1.0000 - val_loss: 11.8821 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0229 - accuracy: 0.9936 - val_loss: 9.5699 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9235e-04 - accuracy: 1.0000 - val_loss: 10.3762 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 9.9917e-05 - accuracy: 1.0000 - val_loss: 10.8028 - val_accuracy: 0.1000 - 3s/epoch - 19ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.5375e-05 - accuracy: 1.0000 - val_loss: 11.1101 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7228e-05 - accuracy: 1.0000 - val_loss: 11.3568 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6145e-05 - accuracy: 1.0000 - val_loss: 11.5657 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8721e-05 - accuracy: 1.0000 - val_loss: 11.7497 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3436e-05 - accuracy: 1.0000 - val_loss: 11.9145 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9511e-05 - accuracy: 1.0000 - val_loss: 12.0639 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6500e-05 - accuracy: 1.0000 - val_loss: 12.2010 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0133 - accuracy: 0.9982 - val_loss: 9.6163 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8461e-04 - accuracy: 1.0000 - val_loss: 10.1914 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0510e-04 - accuracy: 1.0000 - val_loss: 10.5931 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.9838e-05 - accuracy: 1.0000 - val_loss: 10.9030 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.0632e-05 - accuracy: 1.0000 - val_loss: 11.1546 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8807e-05 - accuracy: 1.0000 - val_loss: 11.3678 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0898e-05 - accuracy: 1.0000 - val_loss: 11.5546 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5240e-05 - accuracy: 1.0000 - val_loss: 11.7231 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1030e-05 - accuracy: 1.0000 - val_loss: 11.8762 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7781e-05 - accuracy: 1.0000 - val_loss: 12.0181 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0220 - accuracy: 0.9936 - val_loss: 9.7671 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0225e-04 - accuracy: 1.0000 - val_loss: 10.3929 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1101e-04 - accuracy: 1.0000 - val_loss: 10.8152 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.1734e-05 - accuracy: 1.0000 - val_loss: 11.1338 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.0948e-05 - accuracy: 1.0000 - val_loss: 11.3873 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8490e-05 - accuracy: 1.0000 - val_loss: 11.5979 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0334e-05 - accuracy: 1.0000 - val_loss: 11.7791 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4644e-05 - accuracy: 1.0000 - val_loss: 11.9386 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0490e-05 - accuracy: 1.0000 - val_loss: 12.0821 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7338e-05 - accuracy: 1.0000 - val_loss: 12.2126 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :4\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0259 - accuracy: 0.9938 - val_loss: 9.4425 - val_accuracy: 0.1000 - 5s/epoch - 35ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3604e-04 - accuracy: 1.0000 - val_loss: 10.0807 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2739e-04 - accuracy: 1.0000 - val_loss: 10.5180 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.2563e-05 - accuracy: 1.0000 - val_loss: 10.8298 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.9380e-05 - accuracy: 1.0000 - val_loss: 11.0825 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5043e-05 - accuracy: 1.0000 - val_loss: 11.3015 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5433e-05 - accuracy: 1.0000 - val_loss: 11.4936 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8620e-05 - accuracy: 1.0000 - val_loss: 11.6658 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3607e-05 - accuracy: 1.0000 - val_loss: 11.8233 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 04:12:49,652 | server.py:236 | fit_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:12:49,679 | server.py:173 | evaluate_round 4: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9772e-05 - accuracy: 1.0000 - val_loss: 11.9711 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4877 - accuracy: 0.1280 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4877 - accuracy: 0.1280 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4877 - accuracy: 0.1280 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4877 - accuracy: 0.1280 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4877 - accuracy: 0.1280 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.4877 - accuracy: 0.1280 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4877 - accuracy: 0.1280 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4877 - accuracy: 0.1280 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4877 - accuracy: 0.1280 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:17:16,162 | server.py:187 | evaluate_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:17:16,168 | server.py:222 | fit_round 5: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4877 - accuracy: 0.1280 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0229 - accuracy: 0.9936 - val_loss: 9.7023 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8381e-04 - accuracy: 1.0000 - val_loss: 10.2925 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1124e-04 - accuracy: 1.0000 - val_loss: 10.6309 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.8345e-05 - accuracy: 1.0000 - val_loss: 10.8874 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.9246e-05 - accuracy: 1.0000 - val_loss: 11.1005 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.6792e-05 - accuracy: 1.0000 - val_loss: 11.2846 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8063e-05 - accuracy: 1.0000 - val_loss: 11.4500 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1624e-05 - accuracy: 1.0000 - val_loss: 11.6013 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6695e-05 - accuracy: 1.0000 - val_loss: 11.7403 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2819e-05 - accuracy: 1.0000 - val_loss: 11.8708 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0233 - accuracy: 0.9936 - val_loss: 9.7708 - val_accuracy: 0.1000 - 5s/epoch - 35ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8046e-04 - accuracy: 1.0000 - val_loss: 10.2595 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1344e-04 - accuracy: 1.0000 - val_loss: 10.6079 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.0789e-05 - accuracy: 1.0000 - val_loss: 10.8556 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1543e-05 - accuracy: 1.0000 - val_loss: 11.0647 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8842e-05 - accuracy: 1.0000 - val_loss: 11.2458 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9881e-05 - accuracy: 1.0000 - val_loss: 11.4075 - val_accuracy: 0.1000 - 2s/epoch - 16ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3264e-05 - accuracy: 1.0000 - val_loss: 11.5545 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8202e-05 - accuracy: 1.0000 - val_loss: 11.6898 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4216e-05 - accuracy: 1.0000 - val_loss: 11.8159 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0337 - accuracy: 0.9936 - val_loss: 9.3194 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4825e-04 - accuracy: 1.0000 - val_loss: 9.9376 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4305e-04 - accuracy: 1.0000 - val_loss: 10.3377 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.5765e-05 - accuracy: 1.0000 - val_loss: 10.6559 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.5141e-05 - accuracy: 1.0000 - val_loss: 11.0557 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7500e-05 - accuracy: 1.0000 - val_loss: 11.2521 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8540e-05 - accuracy: 1.0000 - val_loss: 11.4198 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1988e-05 - accuracy: 1.0000 - val_loss: 11.5710 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7003e-05 - accuracy: 1.0000 - val_loss: 11.7093 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3122e-05 - accuracy: 1.0000 - val_loss: 11.8375 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0147 - accuracy: 0.9948 - val_loss: 9.9430 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5922e-04 - accuracy: 1.0000 - val_loss: 10.5198 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.4842e-05 - accuracy: 1.0000 - val_loss: 10.8791 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.5850e-05 - accuracy: 1.0000 - val_loss: 11.1590 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9144e-05 - accuracy: 1.0000 - val_loss: 11.3882 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8374e-05 - accuracy: 1.0000 - val_loss: 11.5855 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0970e-05 - accuracy: 1.0000 - val_loss: 11.7586 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5597e-05 - accuracy: 1.0000 - val_loss: 11.9136 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1539e-05 - accuracy: 1.0000 - val_loss: 12.0553 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8381e-05 - accuracy: 1.0000 - val_loss: 12.1866 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0144 - accuracy: 0.9956 - val_loss: 9.5722 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0245e-04 - accuracy: 1.0000 - val_loss: 10.1347 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1693e-04 - accuracy: 1.0000 - val_loss: 10.5304 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 7.8404e-05 - accuracy: 1.0000 - val_loss: 10.8337 - val_accuracy: 0.1000 - 3s/epoch - 20ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.7190e-05 - accuracy: 1.0000 - val_loss: 11.0811 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.4005e-05 - accuracy: 1.0000 - val_loss: 11.2908 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5104e-05 - accuracy: 1.0000 - val_loss: 11.4737 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8750e-05 - accuracy: 1.0000 - val_loss: 11.6381 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4011e-05 - accuracy: 1.0000 - val_loss: 11.7872 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0370e-05 - accuracy: 1.0000 - val_loss: 11.9251 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0338 - accuracy: 0.9936 - val_loss: 8.8604 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.0613e-04 - accuracy: 1.0000 - val_loss: 9.5145 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2135e-04 - accuracy: 1.0000 - val_loss: 9.9397 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4536e-04 - accuracy: 1.0000 - val_loss: 10.2614 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0481e-04 - accuracy: 1.0000 - val_loss: 10.5207 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.0127e-05 - accuracy: 1.0000 - val_loss: 10.7359 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.3741e-05 - accuracy: 1.0000 - val_loss: 10.9228 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.2135e-05 - accuracy: 1.0000 - val_loss: 11.0885 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.3502e-05 - accuracy: 1.0000 - val_loss: 11.2390 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6875e-05 - accuracy: 1.0000 - val_loss: 11.3771 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0129 - accuracy: 0.9974 - val_loss: 10.1159 - val_accuracy: 0.1000 - 5s/epoch - 32ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3782e-04 - accuracy: 1.0000 - val_loss: 10.6417 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.2072e-05 - accuracy: 1.0000 - val_loss: 10.9960 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.6742e-05 - accuracy: 1.0000 - val_loss: 11.2761 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1902e-05 - accuracy: 1.0000 - val_loss: 11.5424 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2126e-05 - accuracy: 1.0000 - val_loss: 11.7708 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5697e-05 - accuracy: 1.0000 - val_loss: 11.9497 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1171e-05 - accuracy: 1.0000 - val_loss: 12.1066 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7805e-05 - accuracy: 1.0000 - val_loss: 12.2477 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5202e-05 - accuracy: 1.0000 - val_loss: 12.3784 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0150 - accuracy: 0.9952 - val_loss: 10.2095 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3292e-04 - accuracy: 1.0000 - val_loss: 10.7244 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.0035e-05 - accuracy: 1.0000 - val_loss: 11.0704 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.5601e-05 - accuracy: 1.0000 - val_loss: 11.3366 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1484e-05 - accuracy: 1.0000 - val_loss: 11.5610 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2091e-05 - accuracy: 1.0000 - val_loss: 11.7716 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5345e-05 - accuracy: 1.0000 - val_loss: 11.9696 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0523e-05 - accuracy: 1.0000 - val_loss: 12.1402 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7039e-05 - accuracy: 1.0000 - val_loss: 12.2908 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4420e-05 - accuracy: 1.0000 - val_loss: 12.4258 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0160 - accuracy: 0.9952 - val_loss: 9.9689 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6367e-04 - accuracy: 1.0000 - val_loss: 10.5278 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0023e-04 - accuracy: 1.0000 - val_loss: 10.8608 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 7.0755e-05 - accuracy: 1.0000 - val_loss: 11.1196 - val_accuracy: 0.1000 - 3s/epoch - 19ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.3395e-05 - accuracy: 1.0000 - val_loss: 11.3370 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2098e-05 - accuracy: 1.0000 - val_loss: 11.5257 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4138e-05 - accuracy: 1.0000 - val_loss: 11.6953 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8156e-05 - accuracy: 1.0000 - val_loss: 11.8565 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3578e-05 - accuracy: 1.0000 - val_loss: 12.0102 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0050e-05 - accuracy: 1.0000 - val_loss: 12.1473 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :5\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0249 - accuracy: 0.9936 - val_loss: 9.7185 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8783e-04 - accuracy: 1.0000 - val_loss: 10.1858 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1836e-04 - accuracy: 1.0000 - val_loss: 10.5218 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.3007e-05 - accuracy: 1.0000 - val_loss: 10.8003 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1787e-05 - accuracy: 1.0000 - val_loss: 11.0394 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8056e-05 - accuracy: 1.0000 - val_loss: 11.2418 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8672e-05 - accuracy: 1.0000 - val_loss: 11.4208 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1890e-05 - accuracy: 1.0000 - val_loss: 11.5812 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6788e-05 - accuracy: 1.0000 - val_loss: 11.7266 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 04:21:01,036 | server.py:236 | fit_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:21:01,062 | server.py:173 | evaluate_round 5: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2852e-05 - accuracy: 1.0000 - val_loss: 11.8598 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.7362 - accuracy: 0.0842 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.7362 - accuracy: 0.0842 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.7362 - accuracy: 0.0842 - 25s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.7362 - accuracy: 0.0842 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.7362 - accuracy: 0.0842 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.7362 - accuracy: 0.0842 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.7362 - accuracy: 0.0842 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.7362 - accuracy: 0.0842 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.7362 - accuracy: 0.0842 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:25:27,735 | server.py:187 | evaluate_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:25:27,739 | server.py:222 | fit_round 6: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.7362 - accuracy: 0.0842 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0219 - accuracy: 0.9936 - val_loss: 10.0683 - val_accuracy: 0.1000 - 5s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3226e-04 - accuracy: 1.0000 - val_loss: 10.5358 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 8.1564e-05 - accuracy: 1.0000 - val_loss: 10.8676 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.6946e-05 - accuracy: 1.0000 - val_loss: 11.1275 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2719e-05 - accuracy: 1.0000 - val_loss: 11.3442 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3541e-05 - accuracy: 1.0000 - val_loss: 11.5323 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7165e-05 - accuracy: 1.0000 - val_loss: 11.7003 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2505e-05 - accuracy: 1.0000 - val_loss: 11.8532 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8967e-05 - accuracy: 1.0000 - val_loss: 11.9950 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6209e-05 - accuracy: 1.0000 - val_loss: 12.1270 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0295 - accuracy: 0.9932 - val_loss: 9.7594 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8755e-04 - accuracy: 1.0000 - val_loss: 10.2445 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1384e-04 - accuracy: 1.0000 - val_loss: 10.5855 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.8557e-05 - accuracy: 1.0000 - val_loss: 10.8535 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.8286e-05 - accuracy: 1.0000 - val_loss: 11.0838 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5268e-05 - accuracy: 1.0000 - val_loss: 11.2899 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6243e-05 - accuracy: 1.0000 - val_loss: 11.4792 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9885e-05 - accuracy: 1.0000 - val_loss: 11.6407 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5250e-05 - accuracy: 1.0000 - val_loss: 11.7827 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1646e-05 - accuracy: 1.0000 - val_loss: 11.9134 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0243 - accuracy: 0.9936 - val_loss: 9.7225 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0034e-04 - accuracy: 1.0000 - val_loss: 10.3167 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 1.1979e-04 - accuracy: 1.0000 - val_loss: 10.6503 - val_accuracy: 0.1000 - 3s/epoch - 16ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.5236e-05 - accuracy: 1.0000 - val_loss: 10.8864 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.5505e-05 - accuracy: 1.0000 - val_loss: 11.0848 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.2241e-05 - accuracy: 1.0000 - val_loss: 11.2596 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2779e-05 - accuracy: 1.0000 - val_loss: 11.4169 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5657e-05 - accuracy: 1.0000 - val_loss: 11.5621 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0006e-05 - accuracy: 1.0000 - val_loss: 11.6986 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5642e-05 - accuracy: 1.0000 - val_loss: 11.8225 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0324 - accuracy: 0.9928 - val_loss: 9.7524 - val_accuracy: 0.1000 - 5s/epoch - 35ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9721e-04 - accuracy: 1.0000 - val_loss: 10.3218 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1073e-04 - accuracy: 1.0000 - val_loss: 10.7323 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.2303e-05 - accuracy: 1.0000 - val_loss: 11.0428 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1793e-05 - accuracy: 1.0000 - val_loss: 11.2918 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9300e-05 - accuracy: 1.0000 - val_loss: 11.5060 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 3.0524e-05 - accuracy: 1.0000 - val_loss: 11.7163 - val_accuracy: 0.1000 - 3s/epoch - 16ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4005e-05 - accuracy: 1.0000 - val_loss: 11.9322 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9685e-05 - accuracy: 1.0000 - val_loss: 12.0765 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6952e-05 - accuracy: 1.0000 - val_loss: 12.1988 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0233 - accuracy: 0.9936 - val_loss: 9.7380 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7856e-04 - accuracy: 1.0000 - val_loss: 10.3160 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 9.6368e-05 - accuracy: 1.0000 - val_loss: 10.7716 - val_accuracy: 0.1000 - 3s/epoch - 17ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.5442e-05 - accuracy: 1.0000 - val_loss: 11.0392 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8884e-05 - accuracy: 1.0000 - val_loss: 11.2611 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8223e-05 - accuracy: 1.0000 - val_loss: 11.4532 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0864e-05 - accuracy: 1.0000 - val_loss: 11.6247 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5538e-05 - accuracy: 1.0000 - val_loss: 11.7778 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1528e-05 - accuracy: 1.0000 - val_loss: 11.9186 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8408e-05 - accuracy: 1.0000 - val_loss: 12.0478 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0275 - accuracy: 0.9936 - val_loss: 9.6684 - val_accuracy: 0.1000 - 4s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0132e-04 - accuracy: 1.0000 - val_loss: 10.1536 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 1.2410e-04 - accuracy: 1.0000 - val_loss: 10.5187 - val_accuracy: 0.1000 - 3s/epoch - 16ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.5079e-05 - accuracy: 1.0000 - val_loss: 10.8110 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.2603e-05 - accuracy: 1.0000 - val_loss: 11.0544 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8392e-05 - accuracy: 1.0000 - val_loss: 11.2621 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8749e-05 - accuracy: 1.0000 - val_loss: 11.4439 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1878e-05 - accuracy: 1.0000 - val_loss: 11.6064 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6765e-05 - accuracy: 1.0000 - val_loss: 11.7530 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2839e-05 - accuracy: 1.0000 - val_loss: 11.8876 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0141 - accuracy: 0.9940 - val_loss: 10.0165 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3249e-04 - accuracy: 1.0000 - val_loss: 10.5570 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.2729e-05 - accuracy: 1.0000 - val_loss: 10.9114 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.7388e-05 - accuracy: 1.0000 - val_loss: 11.1907 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2669e-05 - accuracy: 1.0000 - val_loss: 11.4224 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3218e-05 - accuracy: 1.0000 - val_loss: 11.6213 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6676e-05 - accuracy: 1.0000 - val_loss: 11.7993 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1928e-05 - accuracy: 1.0000 - val_loss: 11.9602 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8360e-05 - accuracy: 1.0000 - val_loss: 12.1086 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5601e-05 - accuracy: 1.0000 - val_loss: 12.2437 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0124 - accuracy: 0.9948 - val_loss: 9.8952 - val_accuracy: 0.1000 - 5s/epoch - 35ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1853e-04 - accuracy: 1.0000 - val_loss: 10.7339 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 6.6113e-05 - accuracy: 1.0000 - val_loss: 11.1091 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7236e-05 - accuracy: 1.0000 - val_loss: 11.3696 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6239e-05 - accuracy: 1.0000 - val_loss: 11.5818 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8944e-05 - accuracy: 1.0000 - val_loss: 11.7654 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3831e-05 - accuracy: 1.0000 - val_loss: 11.9272 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0032e-05 - accuracy: 1.0000 - val_loss: 12.0724 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7111e-05 - accuracy: 1.0000 - val_loss: 12.2061 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4792e-05 - accuracy: 1.0000 - val_loss: 12.3292 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0099 - accuracy: 0.9970 - val_loss: 10.0045 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2264e-04 - accuracy: 1.0000 - val_loss: 10.5273 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.1744e-05 - accuracy: 1.0000 - val_loss: 10.9175 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9251e-05 - accuracy: 1.0000 - val_loss: 11.1930 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7065e-05 - accuracy: 1.0000 - val_loss: 11.4208 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9091e-05 - accuracy: 1.0000 - val_loss: 11.6214 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3499e-05 - accuracy: 1.0000 - val_loss: 11.8010 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9379e-05 - accuracy: 1.0000 - val_loss: 11.9651 - val_accuracy: 0.1000 - 2s/epoch - 16ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6252e-05 - accuracy: 1.0000 - val_loss: 12.1162 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3825e-05 - accuracy: 1.0000 - val_loss: 12.2566 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :6\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0279 - accuracy: 0.9936 - val_loss: 9.8165 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9212e-04 - accuracy: 1.0000 - val_loss: 10.3098 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 1.1451e-04 - accuracy: 1.0000 - val_loss: 10.6770 - val_accuracy: 0.1000 - 3s/epoch - 21ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.8102e-05 - accuracy: 1.0000 - val_loss: 10.9604 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.7849e-05 - accuracy: 1.0000 - val_loss: 11.1913 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5028e-05 - accuracy: 1.0000 - val_loss: 11.3894 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.6235e-05 - accuracy: 1.0000 - val_loss: 11.5646 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9890e-05 - accuracy: 1.0000 - val_loss: 11.7214 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5127e-05 - accuracy: 1.0000 - val_loss: 11.8658 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 04:29:17,804 | server.py:236 | fit_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 6 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:29:17,828 | server.py:173 | evaluate_round 6: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1428e-05 - accuracy: 1.0000 - val_loss: 11.9995 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.6189 - accuracy: 0.1235 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.6189 - accuracy: 0.1235 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.6189 - accuracy: 0.1235 - 25s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 25s - loss: 2.6189 - accuracy: 0.1235 - 25s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.6189 - accuracy: 0.1235 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.6189 - accuracy: 0.1235 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.6189 - accuracy: 0.1235 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.6189 - accuracy: 0.1235 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.6189 - accuracy: 0.1235 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:33:39,452 | server.py:187 | evaluate_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:33:39,460 | server.py:222 | fit_round 7: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.6189 - accuracy: 0.1235 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0163 - accuracy: 0.9944 - val_loss: 9.9037 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6261e-04 - accuracy: 1.0000 - val_loss: 10.4979 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.3015e-05 - accuracy: 1.0000 - val_loss: 10.8804 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.4833e-05 - accuracy: 1.0000 - val_loss: 11.1383 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9654e-05 - accuracy: 1.0000 - val_loss: 11.3443 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9693e-05 - accuracy: 1.0000 - val_loss: 11.5265 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2474e-05 - accuracy: 1.0000 - val_loss: 11.7004 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7052e-05 - accuracy: 1.0000 - val_loss: 11.8588 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2971e-05 - accuracy: 1.0000 - val_loss: 11.9962 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9791e-05 - accuracy: 1.0000 - val_loss: 12.1205 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0352 - accuracy: 0.9930 - val_loss: 9.4399 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2204e-04 - accuracy: 1.0000 - val_loss: 10.1032 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 1.3701e-04 - accuracy: 1.0000 - val_loss: 10.4099 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.9452e-05 - accuracy: 1.0000 - val_loss: 10.6556 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.6369e-05 - accuracy: 1.0000 - val_loss: 10.8635 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.0819e-05 - accuracy: 1.0000 - val_loss: 11.0436 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9722e-05 - accuracy: 1.0000 - val_loss: 11.2068 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1463e-05 - accuracy: 1.0000 - val_loss: 11.3535 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5143e-05 - accuracy: 1.0000 - val_loss: 11.4883 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0172e-05 - accuracy: 1.0000 - val_loss: 11.6138 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0183 - accuracy: 0.9940 - val_loss: 10.1975 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2101e-04 - accuracy: 1.0000 - val_loss: 10.7679 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.8465e-05 - accuracy: 1.0000 - val_loss: 11.0301 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.9365e-05 - accuracy: 1.0000 - val_loss: 11.2500 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.6721e-05 - accuracy: 1.0000 - val_loss: 11.4435 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7741e-05 - accuracy: 1.0000 - val_loss: 11.6186 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1073e-05 - accuracy: 1.0000 - val_loss: 11.7809 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5950e-05 - accuracy: 1.0000 - val_loss: 11.9313 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1804e-05 - accuracy: 1.0000 - val_loss: 12.0811 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7400e-05 - accuracy: 1.0000 - val_loss: 12.3116 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0194 - accuracy: 0.9936 - val_loss: 9.5360 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0854e-04 - accuracy: 1.0000 - val_loss: 10.1863 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1074e-04 - accuracy: 1.0000 - val_loss: 10.6267 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.1817e-05 - accuracy: 1.0000 - val_loss: 10.9488 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1869e-05 - accuracy: 1.0000 - val_loss: 11.2030 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9911e-05 - accuracy: 1.0000 - val_loss: 11.4160 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1667e-05 - accuracy: 1.0000 - val_loss: 11.6719 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4052e-05 - accuracy: 1.0000 - val_loss: 11.8577 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0613e-05 - accuracy: 1.0000 - val_loss: 11.9820 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7967e-05 - accuracy: 1.0000 - val_loss: 12.0974 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0161 - accuracy: 0.9942 - val_loss: 9.6845 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7384e-04 - accuracy: 1.0000 - val_loss: 10.2074 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 1.1056e-04 - accuracy: 1.0000 - val_loss: 10.5467 - val_accuracy: 0.1000 - 3s/epoch - 17ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.0391e-05 - accuracy: 1.0000 - val_loss: 10.8008 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.2146e-05 - accuracy: 1.0000 - val_loss: 11.0116 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9743e-05 - accuracy: 1.0000 - val_loss: 11.1980 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.0740e-05 - accuracy: 1.0000 - val_loss: 11.3679 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3798e-05 - accuracy: 1.0000 - val_loss: 11.5351 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6153e-05 - accuracy: 1.0000 - val_loss: 11.8345 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1296e-05 - accuracy: 1.0000 - val_loss: 11.9638 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0456 - accuracy: 0.9888 - val_loss: 9.7049 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9120e-04 - accuracy: 1.0000 - val_loss: 10.3055 - val_accuracy: 0.1000 - 2s/epoch - 15ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0773e-04 - accuracy: 1.0000 - val_loss: 10.6743 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.5967e-05 - accuracy: 1.0000 - val_loss: 10.9206 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.8607e-05 - accuracy: 1.0000 - val_loss: 11.1196 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7475e-05 - accuracy: 1.0000 - val_loss: 11.2863 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9573e-05 - accuracy: 1.0000 - val_loss: 11.4334 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3593e-05 - accuracy: 1.0000 - val_loss: 11.5672 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8896e-05 - accuracy: 1.0000 - val_loss: 11.6914 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5113e-05 - accuracy: 1.0000 - val_loss: 11.8072 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0179 - accuracy: 0.9936 - val_loss: 9.6349 - val_accuracy: 0.1000 - 4s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0839e-04 - accuracy: 1.0000 - val_loss: 10.2588 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2053e-04 - accuracy: 1.0000 - val_loss: 10.6566 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 8.3686e-05 - accuracy: 1.0000 - val_loss: 10.9521 - val_accuracy: 0.1000 - 3s/epoch - 22ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.2867e-05 - accuracy: 1.0000 - val_loss: 11.1873 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.9962e-05 - accuracy: 1.0000 - val_loss: 11.3809 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1052e-05 - accuracy: 1.0000 - val_loss: 11.5492 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4439e-05 - accuracy: 1.0000 - val_loss: 11.7017 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9331e-05 - accuracy: 1.0000 - val_loss: 11.8408 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5278e-05 - accuracy: 1.0000 - val_loss: 11.9702 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0238 - accuracy: 0.9930 - val_loss: 10.1341 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2254e-04 - accuracy: 1.0000 - val_loss: 10.6095 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.5918e-05 - accuracy: 1.0000 - val_loss: 10.8618 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.5336e-05 - accuracy: 1.0000 - val_loss: 11.0802 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.1766e-05 - accuracy: 1.0000 - val_loss: 11.2702 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.2152e-05 - accuracy: 1.0000 - val_loss: 11.4393 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5018e-05 - accuracy: 1.0000 - val_loss: 11.5940 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9628e-05 - accuracy: 1.0000 - val_loss: 11.7334 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5435e-05 - accuracy: 1.0000 - val_loss: 11.8617 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2082e-05 - accuracy: 1.0000 - val_loss: 11.9815 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 6s - loss: 0.0132 - accuracy: 0.9958 - val_loss: 10.1975 - val_accuracy: 0.1000 - 6s/epoch - 36ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3467e-04 - accuracy: 1.0000 - val_loss: 10.6306 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 8.5284e-05 - accuracy: 1.0000 - val_loss: 10.9449 - val_accuracy: 0.1000 - 3s/epoch - 17ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1117e-05 - accuracy: 1.0000 - val_loss: 11.1911 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.4519e-05 - accuracy: 1.0000 - val_loss: 11.5579 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2006e-05 - accuracy: 1.0000 - val_loss: 11.7383 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6501e-05 - accuracy: 1.0000 - val_loss: 11.8873 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2467e-05 - accuracy: 1.0000 - val_loss: 12.0210 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9328e-05 - accuracy: 1.0000 - val_loss: 12.1452 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6792e-05 - accuracy: 1.0000 - val_loss: 12.2626 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :7\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0140 - accuracy: 0.9964 - val_loss: 10.4317 - val_accuracy: 0.1000 - 5s/epoch - 30ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0124e-04 - accuracy: 1.0000 - val_loss: 10.8606 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.7148e-05 - accuracy: 1.0000 - val_loss: 11.1397 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.0097e-05 - accuracy: 1.0000 - val_loss: 11.3582 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.9471e-05 - accuracy: 1.0000 - val_loss: 11.5435 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2158e-05 - accuracy: 1.0000 - val_loss: 11.7071 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6821e-05 - accuracy: 1.0000 - val_loss: 11.8553 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2745e-05 - accuracy: 1.0000 - val_loss: 11.9907 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9551e-05 - accuracy: 1.0000 - val_loss: 12.1160 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 04:37:24,995 | server.py:236 | fit_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 7 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:37:25,026 | server.py:173 | evaluate_round 7: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6984e-05 - accuracy: 1.0000 - val_loss: 12.2331 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3667 - accuracy: 0.1228 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3667 - accuracy: 0.1228 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3667 - accuracy: 0.1228 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3667 - accuracy: 0.1228 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3667 - accuracy: 0.1228 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3667 - accuracy: 0.1228 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3667 - accuracy: 0.1228 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3667 - accuracy: 0.1228 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.3667 - accuracy: 0.1228 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:41:50,712 | server.py:187 | evaluate_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:41:50,720 | server.py:222 | fit_round 8: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.3667 - accuracy: 0.1228 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0233 - accuracy: 0.9932 - val_loss: 9.8896 - val_accuracy: 0.1000 - 4s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5965e-04 - accuracy: 1.0000 - val_loss: 10.2935 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0432e-04 - accuracy: 1.0000 - val_loss: 10.6373 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 7.2559e-05 - accuracy: 1.0000 - val_loss: 10.9387 - val_accuracy: 0.1000 - 3s/epoch - 21ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.3690e-05 - accuracy: 1.0000 - val_loss: 11.1921 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1466e-05 - accuracy: 1.0000 - val_loss: 11.4123 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.3049e-05 - accuracy: 1.0000 - val_loss: 11.6098 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6516e-05 - accuracy: 1.0000 - val_loss: 11.8699 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9990e-05 - accuracy: 1.0000 - val_loss: 12.1006 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6893e-05 - accuracy: 1.0000 - val_loss: 12.2207 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0171 - accuracy: 0.9944 - val_loss: 10.0606 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1889e-04 - accuracy: 1.0000 - val_loss: 10.6049 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.6146e-05 - accuracy: 1.0000 - val_loss: 10.9047 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.5052e-05 - accuracy: 1.0000 - val_loss: 11.1641 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1218e-05 - accuracy: 1.0000 - val_loss: 11.4030 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2712e-05 - accuracy: 1.0000 - val_loss: 11.5732 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6979e-05 - accuracy: 1.0000 - val_loss: 11.7312 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2602e-05 - accuracy: 1.0000 - val_loss: 11.8746 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9330e-05 - accuracy: 1.0000 - val_loss: 11.9966 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.6824e-05 - accuracy: 1.0000 - val_loss: 12.1069 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0128 - accuracy: 0.9972 - val_loss: 9.8344 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4414e-04 - accuracy: 1.0000 - val_loss: 10.4925 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 8.6640e-05 - accuracy: 1.0000 - val_loss: 10.7736 - val_accuracy: 0.1000 - 3s/epoch - 18ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.3278e-05 - accuracy: 1.0000 - val_loss: 11.0097 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8477e-05 - accuracy: 1.0000 - val_loss: 11.2127 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8466e-05 - accuracy: 1.0000 - val_loss: 11.3922 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.1347e-05 - accuracy: 1.0000 - val_loss: 11.5525 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.6095e-05 - accuracy: 1.0000 - val_loss: 11.6975 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2107e-05 - accuracy: 1.0000 - val_loss: 11.8317 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8982e-05 - accuracy: 1.0000 - val_loss: 11.9568 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0171 - accuracy: 0.9940 - val_loss: 10.0851 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.2448e-04 - accuracy: 1.0000 - val_loss: 10.6816 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 7.0877e-05 - accuracy: 1.0000 - val_loss: 11.0462 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.0106e-05 - accuracy: 1.0000 - val_loss: 11.3090 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.7916e-05 - accuracy: 1.0000 - val_loss: 11.5234 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.0092e-05 - accuracy: 1.0000 - val_loss: 11.7053 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4664e-05 - accuracy: 1.0000 - val_loss: 11.8631 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0655e-05 - accuracy: 1.0000 - val_loss: 12.0038 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7525e-05 - accuracy: 1.0000 - val_loss: 12.1356 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.4978e-05 - accuracy: 1.0000 - val_loss: 12.2648 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0193 - accuracy: 0.9946 - val_loss: 9.9704 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3400e-04 - accuracy: 1.0000 - val_loss: 10.4536 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.4596e-05 - accuracy: 1.0000 - val_loss: 10.7937 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 6.0331e-05 - accuracy: 1.0000 - val_loss: 11.0299 - val_accuracy: 0.1000 - 4s/epoch - 22ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.7101e-05 - accuracy: 1.0000 - val_loss: 11.2107 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8366e-05 - accuracy: 1.0000 - val_loss: 11.3666 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2022e-05 - accuracy: 1.0000 - val_loss: 11.5085 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7248e-05 - accuracy: 1.0000 - val_loss: 11.6394 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3488e-05 - accuracy: 1.0000 - val_loss: 11.7614 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0454e-05 - accuracy: 1.0000 - val_loss: 11.8779 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0242 - accuracy: 0.9936 - val_loss: 9.8297 - val_accuracy: 0.1000 - 5s/epoch - 33ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7746e-04 - accuracy: 1.0000 - val_loss: 10.2564 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1528e-04 - accuracy: 1.0000 - val_loss: 10.5777 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.2521e-05 - accuracy: 1.0000 - val_loss: 10.8423 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.2962e-05 - accuracy: 1.0000 - val_loss: 11.0476 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.0499e-05 - accuracy: 1.0000 - val_loss: 11.2250 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1483e-05 - accuracy: 1.0000 - val_loss: 11.3949 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4724e-05 - accuracy: 1.0000 - val_loss: 11.5493 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9543e-05 - accuracy: 1.0000 - val_loss: 11.6844 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5458e-05 - accuracy: 1.0000 - val_loss: 11.8095 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0217 - accuracy: 0.9938 - val_loss: 10.3366 - val_accuracy: 0.1000 - 5s/epoch - 34ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.0052e-04 - accuracy: 1.0000 - val_loss: 10.8583 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 3s - loss: 6.7231e-05 - accuracy: 1.0000 - val_loss: 11.0546 - val_accuracy: 0.1000 - 3s/epoch - 17ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.3176e-05 - accuracy: 1.0000 - val_loss: 11.2355 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.3006e-05 - accuracy: 1.0000 - val_loss: 11.4010 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5448e-05 - accuracy: 1.0000 - val_loss: 11.5560 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.9686e-05 - accuracy: 1.0000 - val_loss: 11.6990 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5193e-05 - accuracy: 1.0000 - val_loss: 11.8325 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1631e-05 - accuracy: 1.0000 - val_loss: 11.9566 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.8756e-05 - accuracy: 1.0000 - val_loss: 12.0740 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0283 - accuracy: 0.9924 - val_loss: 9.4275 - val_accuracy: 0.1000 - 4s/epoch - 27ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7691e-04 - accuracy: 1.0000 - val_loss: 9.8637 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5607e-04 - accuracy: 1.0000 - val_loss: 10.4646 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.3187e-05 - accuracy: 1.0000 - val_loss: 10.8826 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.5921e-05 - accuracy: 1.0000 - val_loss: 11.1087 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5817e-05 - accuracy: 1.0000 - val_loss: 11.2678 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8520e-05 - accuracy: 1.0000 - val_loss: 11.4089 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2967e-05 - accuracy: 1.0000 - val_loss: 11.5382 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8515e-05 - accuracy: 1.0000 - val_loss: 11.6594 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.4925e-05 - accuracy: 1.0000 - val_loss: 11.7717 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0192 - accuracy: 0.9940 - val_loss: 9.9706 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.3598e-04 - accuracy: 1.0000 - val_loss: 10.4628 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.0446e-05 - accuracy: 1.0000 - val_loss: 10.9244 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 5.5474e-05 - accuracy: 1.0000 - val_loss: 11.2041 - val_accuracy: 0.1000 - 4s/epoch - 24ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.1659e-05 - accuracy: 1.0000 - val_loss: 11.4441 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2855e-05 - accuracy: 1.0000 - val_loss: 11.6342 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7087e-05 - accuracy: 1.0000 - val_loss: 11.7874 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.2985e-05 - accuracy: 1.0000 - val_loss: 11.9211 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.9860e-05 - accuracy: 1.0000 - val_loss: 12.0403 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7364e-05 - accuracy: 1.0000 - val_loss: 12.1501 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :8\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0147 - accuracy: 0.9956 - val_loss: 10.5801 - val_accuracy: 0.1000 - 4s/epoch - 28ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 9.2064e-05 - accuracy: 1.0000 - val_loss: 10.9294 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.1624e-05 - accuracy: 1.0000 - val_loss: 11.1789 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.5442e-05 - accuracy: 1.0000 - val_loss: 11.3836 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.5430e-05 - accuracy: 1.0000 - val_loss: 11.5612 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8651e-05 - accuracy: 1.0000 - val_loss: 11.7189 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3816e-05 - accuracy: 1.0000 - val_loss: 11.8607 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0219e-05 - accuracy: 1.0000 - val_loss: 11.9886 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7441e-05 - accuracy: 1.0000 - val_loss: 12.1067 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-02-12 04:45:36,501 | server.py:236 | fit_round 8 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 8 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:45:36,529 | server.py:173 | evaluate_round 8: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5225e-05 - accuracy: 1.0000 - val_loss: 12.2169 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4544 - accuracy: 0.1047 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4544 - accuracy: 0.1047 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4544 - accuracy: 0.1047 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4544 - accuracy: 0.1047 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4544 - accuracy: 0.1047 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4544 - accuracy: 0.1047 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4544 - accuracy: 0.1047 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4544 - accuracy: 0.1047 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 24s - loss: 2.4544 - accuracy: 0.1047 - 24s/epoch - 2ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n",
            "DEBUG flwr 2024-02-12 04:50:04,733 | server.py:187 | evaluate_round 8 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 10 results and 0 failures\n",
            "DEBUG flwr 2024-02-12 04:50:04,736 | server.py:222 | fit_round 9: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 10 clients (out of 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 10000/10000 - 26s - loss: 2.4544 - accuracy: 0.1047 - 26s/epoch - 3ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Global Validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :9\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 5s - loss: 0.0206 - accuracy: 0.9938 - val_loss: 9.8550 - val_accuracy: 0.1000 - 5s/epoch - 31ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.5108e-04 - accuracy: 1.0000 - val_loss: 10.4898 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 8.8251e-05 - accuracy: 1.0000 - val_loss: 10.8112 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.4450e-05 - accuracy: 1.0000 - val_loss: 11.0533 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 5.0233e-05 - accuracy: 1.0000 - val_loss: 11.2493 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.0752e-05 - accuracy: 1.0000 - val_loss: 11.4178 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.4029e-05 - accuracy: 1.0000 - val_loss: 11.5649 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.8990e-05 - accuracy: 1.0000 - val_loss: 11.6982 - val_accuracy: 0.1000 - 2s/epoch - 13ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.5052e-05 - accuracy: 1.0000 - val_loss: 11.8210 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.1879e-05 - accuracy: 1.0000 - val_loss: 11.9361 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :9\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 4s - loss: 0.0205 - accuracy: 0.9938 - val_loss: 10.0990 - val_accuracy: 0.1000 - 4s/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 2/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.1782e-04 - accuracy: 1.0000 - val_loss: 10.5115 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 3/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 6.8130e-05 - accuracy: 1.0000 - val_loss: 11.0094 - val_accuracy: 0.1000 - 2s/epoch - 12ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 4/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 4.8811e-05 - accuracy: 1.0000 - val_loss: 11.2494 - val_accuracy: 0.1000 - 2s/epoch - 14ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 5/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.8878e-05 - accuracy: 1.0000 - val_loss: 11.4350 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 6/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 3.2201e-05 - accuracy: 1.0000 - val_loss: 11.5906 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 7/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.7268e-05 - accuracy: 1.0000 - val_loss: 11.7273 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 8/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.3447e-05 - accuracy: 1.0000 - val_loss: 11.8542 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 9/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 2.0384e-05 - accuracy: 1.0000 - val_loss: 11.9731 - val_accuracy: 0.1000 - 2s/epoch - 10ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 10/10\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m 157/157 - 2s - loss: 1.7870e-05 - accuracy: 1.0000 - val_loss: 12.0870 - val_accuracy: 0.1000 - 2s/epoch - 11ms/step\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Client Train and validate Done!!!\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m round :9\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Train :5000\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Test :10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=8846)\u001b[0m Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR flwr 2024-02-12 04:50:53,182 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:53,203 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-02-12 04:50:54,081 E 8725 8725] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:54,227 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:54,239 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:54,584 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:54,586 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:54,588 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:54,594 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:54,608 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:54,610 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:55,290 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:55,293 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:55,293 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:55,295 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:55,298 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:55,303 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-12 04:50:55,318 | server.py:236 | fit_round 9 received 2 results and 8 failures\n",
            "DEBUG:flwr:fit_round 9 received 2 results and 8 failures\n",
            "DEBUG flwr 2024-02-12 04:50:55,327 | server.py:173 | evaluate_round 9: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 10 clients (out of 10)\n",
            "ERROR flwr 2024-02-12 04:50:56,308 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:56,313 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:56,641 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:56,650 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:56,650 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:56,659 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:57,003 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:57,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:58,231 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:58,234 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:58,234 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:58,235 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:58,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:58,257 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:58,260 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:58,268 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:58,616 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:58,622 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:58,628 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:58,630 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-12 04:50:58,642 | server.py:187 | evaluate_round 9 received 0 results and 10 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 0 results and 10 failures\n",
            "DEBUG flwr 2024-02-12 04:50:58,644 | server.py:222 | fit_round 10: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 10 clients (out of 10)\n",
            "ERROR flwr 2024-02-12 04:50:59,585 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:59,589 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:50:59,959 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:50:59,964 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:00,376 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:00,384 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:01,120 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:01,134 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:01,888 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:01,892 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:01,892 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:01,893 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:01,906 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:01,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:01,916 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:01,920 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:02,399 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:02,405 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:02,403 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:02,415 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-12 04:51:02,431 | server.py:236 | fit_round 10 received 0 results and 10 failures\n",
            "DEBUG:flwr:fit_round 10 received 0 results and 10 failures\n",
            "DEBUG flwr 2024-02-12 04:51:02,434 | server.py:173 | evaluate_round 10: strategy sampled 10 clients (out of 10)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 10 clients (out of 10)\n",
            "ERROR flwr 2024-02-12 04:51:03,910 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:03,912 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:03,918 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:03,920 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:04,566 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:04,574 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:04,898 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:04,900 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:04,907 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:04,911 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:05,560 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:05,567 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:05,892 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:05,895 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:05,901 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:05,903 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:06,246 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:06,249 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
            "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
            "    res_cid, res, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2024-02-12 04:51:06,251 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2024-02-12 04:51:06,254 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: cc76958743e8a99339f7347bc73f3899e34c02f4d1a40909f19635cf) where the task (actor ID: 9551607ed23016976701cc6c01000000, name=DefaultActor.__init__, pid=8846, memory used=3.90GB) was running was 12.05GB / 12.67GB (0.950803), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-8d3eb198e4829209534cf42a1380e4408c1a67257fb7219dedac1036*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "8846\t3.90\tray::DefaultActor.run\n",
            "2273\t3.59\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6063d1ad-90b9...\n",
            "2688\t0.46\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:0b7bb4d8ec3ccf303...\n",
            "104\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "8763\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "8695\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "8694\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "8740\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "8670\t0.03\t/usr/local/lib/python3.10/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n",
            "58\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2024-02-12 04:51:06,266 | server.py:187 | evaluate_round 10 received 0 results and 10 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 0 results and 10 failures\n",
            "INFO flwr 2024-02-12 04:51:06,268 | server.py:153 | FL finished in 3963.3884643779998\n",
            "INFO:flwr:FL finished in 3963.3884643779998\n",
            "INFO flwr 2024-02-12 04:51:06,270 | app.py:226 | app_fit: losses_distributed [(1, 2.3924214839935303), (2, 2.4202446937561035), (3, 2.3659677505493164), (4, 2.4876580238342285), (5, 2.7362465858459473), (6, 2.618863105773926), (7, 2.366676092147827), (8, 2.45443058013916)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 2.3924214839935303), (2, 2.4202446937561035), (3, 2.3659677505493164), (4, 2.4876580238342285), (5, 2.7362465858459473), (6, 2.618863105773926), (7, 2.366676092147827), (8, 2.45443058013916)]\n",
            "INFO flwr 2024-02-12 04:51:06,275 | app.py:227 | app_fit: metrics_distributed_fit {'example': [(1, 50000), (2, 50000), (3, 50000), (4, 50000), (5, 50000), (6, 50000), (7, 50000), (8, 50000), (9, 10000)], 'accuracy': [(1, 100.0), (2, 100.0), (3, 100.0), (4, 100.0), (5, 100.0), (6, 100.0), (7, 100.0), (8, 100.0), (9, 100.0)], 'valacc': [(1, 10.0), (2, 10.0), (3, 10.0), (4, 10.0), (5, 10.0), (6, 10.0), (7, 10.0), (8, 10.0), (9, 10.0)]}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {'example': [(1, 50000), (2, 50000), (3, 50000), (4, 50000), (5, 50000), (6, 50000), (7, 50000), (8, 50000), (9, 10000)], 'accuracy': [(1, 100.0), (2, 100.0), (3, 100.0), (4, 100.0), (5, 100.0), (6, 100.0), (7, 100.0), (8, 100.0), (9, 100.0)], 'valacc': [(1, 10.0), (2, 10.0), (3, 10.0), (4, 10.0), (5, 10.0), (6, 10.0), (7, 10.0), (8, 10.0), (9, 10.0)]}\n",
            "INFO flwr 2024-02-12 04:51:06,276 | app.py:228 | app_fit: metrics_distributed {'accuracy': [(1, 11.35), (2, 12.89), (3, 12.78), (4, 12.8), (5, 8.42), (6, 12.35), (7, 12.28), (8, 10.47)]}\n",
            "INFO:flwr:app_fit: metrics_distributed {'accuracy': [(1, 11.35), (2, 12.89), (3, 12.78), (4, 12.8), (5, 8.42), (6, 12.35), (7, 12.28), (8, 10.47)]}\n",
            "INFO flwr 2024-02-12 04:51:06,278 | app.py:229 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2024-02-12 04:51:06,283 | app.py:230 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n"
          ]
        }
      ],
      "source": [
        "history = fl.simulation.start_simulation(\n",
        "        client_fn=clientfn,\n",
        "        num_clients=NumOfPartition,\n",
        "        config=fl.server.ServerConfig(num_rounds=10),\n",
        "        strategy=Fedavgconfig,\n",
        "        ray_init_args ={\n",
        "            \"include_dashboard\": True, # we need this one for tracking,\n",
        "        },\n",
        "        actor_kwargs={\n",
        "        \"on_actor_init_fn\": enable_tf_gpu_growth # <-- To be executed upon actor init.\n",
        "    },\n",
        "        client_resources = {'num_cpus': 2, 'num_gpus': 1}, # A Python dict specifying CPU/GPU resources\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rinUEM1vV3fP"
      },
      "outputs": [],
      "source": [
        "Train_acc_result = [x[1] for x in history.metrics_distributed_fit['accuracy']]\n",
        "Val_acc_result = [x[1] for x in history.metrics_distributed_fit['valacc']]\n",
        "Test_acc_result = [x[1] for x in history.metrics_distributed['accuracy']]\n",
        "iteration = [i for i in range(1,len(Train_acc_result)+1)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Val_acc_result= [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n",
        "Test_acc_result =[11.35, 12.89, 12.78, 12.8, 8.42, 12.35, 12.28, 10.47, 11.67]"
      ],
      "metadata": {
        "id": "2eHJDFz5rGyF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "E1EZcaTAV3fP",
        "outputId": "9493ebe3-f320-436d-f178-3e7049ef5dee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local evalution :[10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n",
            "global evalution :[11.35, 12.89, 12.78, 12.8, 8.42, 12.35, 12.28, 10.47, 11.67]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Local evalution :{Val_acc_result}\")\n",
        "print(f\"global evalution :{Test_acc_result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wohX7ATnV3fP",
        "outputId": "8ef88e64-2ab0-49b4-c8c4-fc288dc2db3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#plt.plot(iteration,Train_acc_result,marker='o',label=\"Train\")\n",
        "plt.title(f\"CIFAR Partition/Client {NumOfPartition} Accuracy\")\n",
        "plt.plot(iteration,Val_acc_result,marker='o',label=\"Local evalution\")\n",
        "plt.plot(iteration,Test_acc_result,marker='o',label=\"Aggregation\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(iteration)\n",
        "\n",
        "for i, j in zip(iteration, Val_acc_result):\n",
        "    plt.text(i, j,f\"{round(j,2)}\", ha='left', va='bottom')\n",
        "for i, j in zip(iteration, Test_acc_result):\n",
        "    plt.text(i, j,f\"{round(j,2)}\", ha='left', va='bottom')\n",
        "\n",
        "average_value = 55.00\n",
        "plt.gca().text(0, average_value/60, f\"{average_value}\", transform=plt.gca().transAxes, ha='right',color='r',)\n",
        "plt.axhline(y=average_value, color='r', linestyle='--', label='Centralised model')\n",
        "\n",
        "#plt.savefig(f\"MNIST_Partition_Client_{NumOfPartition}_Accuracy.png\")\n",
        "plt.legend()\n",
        "\n",
        "#plt.savefig(f\"MNIST_Partition_Client_{NumOfPartition}_Accuracy.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhgCr8CRV3fP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f67caa4bb70d417ca8f95dec68ea56a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa716e3d81a24313a8348b71396bebb8",
              "IPY_MODEL_2de9200a359941ecb9eaf4e014784368",
              "IPY_MODEL_2593234433b742f59ffb279360972ed4"
            ],
            "layout": "IPY_MODEL_34c7700212e3429bb27a22b9bf3558fd"
          }
        },
        "aa716e3d81a24313a8348b71396bebb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec6e7ccd6bc4eecac23b7da472edaa8",
            "placeholder": "​",
            "style": "IPY_MODEL_78e84ddb2db348488dc3e551d075e718",
            "value": "Dl Completed...: 100%"
          }
        },
        "2de9200a359941ecb9eaf4e014784368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06e5ee7bae6442dda9ea831f8e762881",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b2e54acd915432a8d265d5b8fef5192",
            "value": 1
          }
        },
        "2593234433b742f59ffb279360972ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02bb9e7765b34cc0a534c5b09be0a9d8",
            "placeholder": "​",
            "style": "IPY_MODEL_23115e2a727b425d955a063e8c577e7d",
            "value": " 1/1 [00:00&lt;00:00,  3.75 url/s]"
          }
        },
        "34c7700212e3429bb27a22b9bf3558fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec6e7ccd6bc4eecac23b7da472edaa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78e84ddb2db348488dc3e551d075e718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06e5ee7bae6442dda9ea831f8e762881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7b2e54acd915432a8d265d5b8fef5192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02bb9e7765b34cc0a534c5b09be0a9d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23115e2a727b425d955a063e8c577e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b029d230fc6841bf958fe62008c287b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7151dd7d59c482582b96208421544ee",
              "IPY_MODEL_cc09c3bc809a411aaadc18c90a7ab8a3",
              "IPY_MODEL_45e07534d4784bd1b2584161687641d0"
            ],
            "layout": "IPY_MODEL_405fff3894f749cb8a6004b626da43d5"
          }
        },
        "f7151dd7d59c482582b96208421544ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc46c385f9084eaaaaf4fa267d0ddf18",
            "placeholder": "​",
            "style": "IPY_MODEL_2c93b55be9674e0ea6e81d909e2ff691",
            "value": "Dl Size...: 100%"
          }
        },
        "cc09c3bc809a411aaadc18c90a7ab8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff250aaf9faf49a5825ea3771efef8e9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2163f8c61f774ba9ab807833580cad1d",
            "value": 1
          }
        },
        "45e07534d4784bd1b2584161687641d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c07efe02104511a4694382ef9a64ce",
            "placeholder": "​",
            "style": "IPY_MODEL_ec49346a96a44e1fb385cba80eeb5cad",
            "value": " 170052171/170052171 [00:00&lt;00:00, 1058406331.34 MiB/s]"
          }
        },
        "405fff3894f749cb8a6004b626da43d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc46c385f9084eaaaaf4fa267d0ddf18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c93b55be9674e0ea6e81d909e2ff691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff250aaf9faf49a5825ea3771efef8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2163f8c61f774ba9ab807833580cad1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45c07efe02104511a4694382ef9a64ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec49346a96a44e1fb385cba80eeb5cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d1fe72e89b34a3393c997a199472025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2aa07bb54c0b4adaa157e7656e238f66",
              "IPY_MODEL_cba1e40517ae43fcab0040d51f8d953c",
              "IPY_MODEL_6c4da78ad379407caac7117313aae9e0"
            ],
            "layout": "IPY_MODEL_8fd070ce820244f8a837c49ea375a92e"
          }
        },
        "2aa07bb54c0b4adaa157e7656e238f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a42d8287c9429f930dfc420ddecac5",
            "placeholder": "​",
            "style": "IPY_MODEL_05895b42051943c086a9fea62392f8ec",
            "value": "Extraction completed...: "
          }
        },
        "cba1e40517ae43fcab0040d51f8d953c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d60d20889a4f059a20045473d072e9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aec203a57e4347e0a6fcc91bfce218ed",
            "value": 0
          }
        },
        "6c4da78ad379407caac7117313aae9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d9dbaad4aa460eb3f0520d78f231e0",
            "placeholder": "​",
            "style": "IPY_MODEL_6034510f50f14b2687d76d9a3b9f5851",
            "value": " 0/0 [00:00&lt;?, ? file/s]"
          }
        },
        "8fd070ce820244f8a837c49ea375a92e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a42d8287c9429f930dfc420ddecac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05895b42051943c086a9fea62392f8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52d60d20889a4f059a20045473d072e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "aec203a57e4347e0a6fcc91bfce218ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06d9dbaad4aa460eb3f0520d78f231e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6034510f50f14b2687d76d9a3b9f5851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fca8b13e7793406190eb4aaad9ed2a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_188ffdde4e274022a8553ebe77a2c438",
              "IPY_MODEL_7253d3af22a44d7ba186f4604a44a603",
              "IPY_MODEL_ff7f1e9e3b8149aabd6274ad16de49df"
            ],
            "layout": "IPY_MODEL_569a575c23044b88bf0bfc257fd4e084"
          }
        },
        "188ffdde4e274022a8553ebe77a2c438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02f9f7e73a904f13a4eb86c07785eb71",
            "placeholder": "​",
            "style": "IPY_MODEL_fbda1a6d1a0b454da9b565a72b9be49b",
            "value": "Generating splits...: 100%"
          }
        },
        "7253d3af22a44d7ba186f4604a44a603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3f9c467fcb4d709d4fba1d3478cf39",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_641b46737fdc4abfbdbbeefce69efcc1",
            "value": 2
          }
        },
        "ff7f1e9e3b8149aabd6274ad16de49df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f596042cc274621bcb38610419c82a0",
            "placeholder": "​",
            "style": "IPY_MODEL_29d6cfff5ecd4dc492d48e7d021e8f00",
            "value": " 2/2 [01:05&lt;00:00, 28.69s/ splits]"
          }
        },
        "569a575c23044b88bf0bfc257fd4e084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "02f9f7e73a904f13a4eb86c07785eb71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbda1a6d1a0b454da9b565a72b9be49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c3f9c467fcb4d709d4fba1d3478cf39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641b46737fdc4abfbdbbeefce69efcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f596042cc274621bcb38610419c82a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d6cfff5ecd4dc492d48e7d021e8f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2afa12ac215244d1bf0f557f90ad0ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63f2f97cfb46483f8d6af148e1596beb",
              "IPY_MODEL_80dfc07a4da64adf81d1e9280ee85932",
              "IPY_MODEL_de86a2e034c04b90b8682667574b0b97"
            ],
            "layout": "IPY_MODEL_fa513f703f764d1cb37965d23eaecc5f"
          }
        },
        "63f2f97cfb46483f8d6af148e1596beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bee3a112e494c0a90ff2e5cd8a059cb",
            "placeholder": "​",
            "style": "IPY_MODEL_8c508789f68243748fa0ea74f9303fb7",
            "value": "Generating train examples...:  99%"
          }
        },
        "80dfc07a4da64adf81d1e9280ee85932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd76fbe28eec4c43a5cb8c3c54d594b3",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed2742f07fe042869f8825acca9e2b16",
            "value": 50000
          }
        },
        "de86a2e034c04b90b8682667574b0b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed2adfd49b143bc82868e67e3340a11",
            "placeholder": "​",
            "style": "IPY_MODEL_46b2e1302a9048a18894a5a67c343034",
            "value": " 49598/50000 [00:55&lt;00:00, 1155.81 examples/s]"
          }
        },
        "fa513f703f764d1cb37965d23eaecc5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "1bee3a112e494c0a90ff2e5cd8a059cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c508789f68243748fa0ea74f9303fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd76fbe28eec4c43a5cb8c3c54d594b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2742f07fe042869f8825acca9e2b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ed2adfd49b143bc82868e67e3340a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b2e1302a9048a18894a5a67c343034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34c35fd2e02b4a1eaad91fda32d70673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d2a4658ade04437946cc08dd18c7cf8",
              "IPY_MODEL_cb8de5adf4204c8ba410bd897fa6828f",
              "IPY_MODEL_b9d40ec0f9ea4f47bf17a1249d5338e3"
            ],
            "layout": "IPY_MODEL_ca97a277372c48508273ed403ee40386"
          }
        },
        "4d2a4658ade04437946cc08dd18c7cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_620c4ed423624ee39a9fc2b2e94aba55",
            "placeholder": "​",
            "style": "IPY_MODEL_75a698042013485db6013b46719a362b",
            "value": "Shuffling /root/tensorflow_datasets/cifar10/3.0.2.incompleteZHGCAF/cifar10-train.tfrecord*...:  75%"
          }
        },
        "cb8de5adf4204c8ba410bd897fa6828f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8227378398453987db29c79c15e5f0",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b8bdad723154e3ca712bc6df3248f79",
            "value": 50000
          }
        },
        "b9d40ec0f9ea4f47bf17a1249d5338e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_007ba4e2373f48e18956ba6130e5d421",
            "placeholder": "​",
            "style": "IPY_MODEL_13070cbe16724c8e9982c06dc6d4e22e",
            "value": " 37740/50000 [00:00&lt;00:00, 140943.08 examples/s]"
          }
        },
        "ca97a277372c48508273ed403ee40386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "620c4ed423624ee39a9fc2b2e94aba55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a698042013485db6013b46719a362b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd8227378398453987db29c79c15e5f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8bdad723154e3ca712bc6df3248f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "007ba4e2373f48e18956ba6130e5d421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13070cbe16724c8e9982c06dc6d4e22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f898fc464b1c49018fbcd9ae72648c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7308710cbfd45a9af7a272014920f03",
              "IPY_MODEL_fd3b7c5e8aba4453960bbca2caf9d712",
              "IPY_MODEL_26d970865de74f3d9d1a6263e0a7b340"
            ],
            "layout": "IPY_MODEL_4a6b141318af425bac8006eeb8c381ce"
          }
        },
        "d7308710cbfd45a9af7a272014920f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5e21744327942e1af6458a190f784f8",
            "placeholder": "​",
            "style": "IPY_MODEL_db66c0e67d284555bcec640af2418856",
            "value": "Generating test examples...:  92%"
          }
        },
        "fd3b7c5e8aba4453960bbca2caf9d712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82301467a6ad4b0eac2e6363e35ffa2d",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5d8cd7702294e32a85f5531ba76cb05",
            "value": 10000
          }
        },
        "26d970865de74f3d9d1a6263e0a7b340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d01f6156c5024f69ab25bc44a5510e45",
            "placeholder": "​",
            "style": "IPY_MODEL_f2e8f66131f44d8fb5743d9ac0e125ab",
            "value": " 9172/10000 [00:08&lt;00:00, 1083.09 examples/s]"
          }
        },
        "4a6b141318af425bac8006eeb8c381ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "e5e21744327942e1af6458a190f784f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db66c0e67d284555bcec640af2418856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82301467a6ad4b0eac2e6363e35ffa2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d8cd7702294e32a85f5531ba76cb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d01f6156c5024f69ab25bc44a5510e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e8f66131f44d8fb5743d9ac0e125ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef36693bae7f40b9b7c733d0ef8a828b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_255d24d087784f0d9f9838573a2ad8b5",
              "IPY_MODEL_36c1b7cd10774474bb055c6cc7c24e15",
              "IPY_MODEL_b27f601bba3a4d39ab576ebe97e930aa"
            ],
            "layout": "IPY_MODEL_26cd62668f794512a0ba325d052339dc"
          }
        },
        "255d24d087784f0d9f9838573a2ad8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b87a4fb1534b87a854f7799826fae2",
            "placeholder": "​",
            "style": "IPY_MODEL_458fbf087baa430a97874adbbb50e23f",
            "value": "Shuffling /root/tensorflow_datasets/cifar10/3.0.2.incompleteZHGCAF/cifar10-test.tfrecord*...:   0%"
          }
        },
        "36c1b7cd10774474bb055c6cc7c24e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a862c943c859472bb05230a78880dce4",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc7b9e9f66f340f5ae5e5c0a4b2a273a",
            "value": 10000
          }
        },
        "b27f601bba3a4d39ab576ebe97e930aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7181073e976942388e12662c90e14d2c",
            "placeholder": "​",
            "style": "IPY_MODEL_8e40f480f0904d7a9cc6072f961dc92f",
            "value": " 0/10000 [00:00&lt;?, ? examples/s]"
          }
        },
        "26cd62668f794512a0ba325d052339dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "e2b87a4fb1534b87a854f7799826fae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "458fbf087baa430a97874adbbb50e23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a862c943c859472bb05230a78880dce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc7b9e9f66f340f5ae5e5c0a4b2a273a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7181073e976942388e12662c90e14d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e40f480f0904d7a9cc6072f961dc92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}